{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample train entry: ['amusement', 'image/amusement/amusement_08258.jpg', 'annotation/amusement/amusement_08258.json']\n",
            "Sample val entry: ['amusement', 'image/amusement/amusement_03317.jpg', 'annotation/amusement/amusement_03317.json']\n",
            "Sample test entry: ['awe', 'image/awe/awe_06628.jpg', 'annotation/awe/awe_06628.json']\n",
            "Total train images: 94481\n",
            "Total validation images: 5905\n",
            "Total test images: 17716\n"
          ]
        }
      ],
      "source": [
        "import os,json\n",
        "data_root=\"C:/Users/HP/Downloads/Emotion-Analysis/dataset\"\n",
        "def load_json(filename):\n",
        "    \"\"\"Load JSON file and return data.\"\"\"\n",
        "    with open(filename, \"r\") as f:\n",
        "        return json.load(f)\n",
        "train_data = load_json(f\"{data_root}/train.json\")\n",
        "val_data = load_json(f\"{data_root}/val.json\")\n",
        "test_data = load_json(f\"{data_root}/test.json\")\n",
        "print(\"Sample train entry:\", train_data[0]) \n",
        "print(\"Sample val entry:\", val_data[0])\n",
        "print(\"Sample test entry:\", test_data[0])\n",
        "def fix_path(path):\n",
        "    return os.path.normpath(os.path.join(data_root, path)).replace(\"\\\\\", \"/\")\n",
        "train_images = [(entry[0], fix_path(entry[1])) for entry in train_data]\n",
        "val_images = [(entry[0], fix_path(entry[1])) for entry in val_data]\n",
        "test_images = [(entry[0], fix_path(entry[1])) for entry in test_data]\n",
        "print(f\"Total train images: {len(train_images)}\")\n",
        "print(f\"Total validation images: {len(val_images)}\")\n",
        "print(f\"Total test images: {len(test_images)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x000002868CDB0A00>\n",
            "tensor([0, 7, 3, 1, 7, 7, 7, 5, 0, 1, 2, 2, 3, 0, 7, 3, 1, 3, 7, 3, 4, 4, 7, 1,\n",
            "        1, 2, 6, 4, 5, 3, 0, 2, 3, 2, 7, 0, 3, 1, 4, 3, 4, 6, 5, 5, 6, 4, 1, 2,\n",
            "        4, 2, 6, 3, 0, 7, 0, 3, 2, 3, 6, 2, 1, 7, 1, 0, 3, 3, 0, 1, 5, 7, 1, 2,\n",
            "        0, 1, 4, 5, 2, 2, 0, 1, 2, 1, 1, 2, 0, 1, 6, 7, 5, 1, 1, 5, 7, 3, 0, 2,\n",
            "        7, 6, 2, 1, 6, 4, 3, 0, 7, 1, 3, 3, 7, 0, 1, 6, 5, 2, 2, 1, 1, 7, 7, 0,\n",
            "        6, 3, 2, 4, 4, 5, 3, 1, 1, 6, 1, 6, 1, 2, 0, 2, 3, 3, 7, 2, 1, 1, 2, 2,\n",
            "        5, 3, 1, 4, 5, 7, 4, 2, 3, 4, 7, 3, 7, 6, 2, 3, 7, 7, 2, 1, 7, 0, 0, 5,\n",
            "        1, 7, 3, 0, 4, 7, 5, 4, 7, 5, 0, 5, 6, 6, 4, 0, 7, 1, 0, 3, 5, 2, 0, 5,\n",
            "        6, 3, 3, 2, 1, 7, 0, 6, 1, 7, 5, 0, 6, 2, 7, 0, 2, 1, 0, 7, 1, 4, 3, 4,\n",
            "        7, 5, 3, 6, 4, 1, 0, 0, 6, 2, 7, 3, 3, 3, 0, 6, 2, 5, 4, 7, 6, 1, 3, 3,\n",
            "        0, 2, 5, 3, 0, 6, 1, 2, 6, 4, 2, 4, 0, 6, 6, 7])\n",
            "tensor([  4,  57, 104,  -1, 144,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         57, 102,  -1,  62,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1, 171,  -1,  -1,  -1,  -1,  97, 238,  39, 249,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 102, 213,  -1,  -1,  -1,\n",
            "         -1, 216,  -1,  -1,  81,  -1,  -1,  -1,  -1,  -1,   4,  -1,  -1, 214,\n",
            "         -1,  -1,  -1,  15,  -1,  -1,  -1,  -1,  -1,  97,  -1,  -1, 142,  -1,\n",
            "         -1, 155,  -1,  57,  -1, 148,  -1,  -1,  -1, 182,  -1, 144,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,   9,  -1,  -1,  -1, 207,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  97,  -1,  -1,  -1,\n",
            "         -1, 157, 185,  -1, 157,  -1, 208,  99,  -1,  -1,  48,  -1,  -1,  -1,\n",
            "        157,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 203,  -1,  -1,  -1,  -1,\n",
            "         -1, 103,  -1,  -1,  -1, 208, 208,  -1, 171, 178,  -1, 208,  -1,  -1,\n",
            "        142,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  57,  -1,   4, 186,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 142,\n",
            "        157,  57,  -1,  -1, 142,  -1,  -1,  -1,  -1,  32,  -1,  -1,  -1, 247,\n",
            "         -1,  -1,  -1, 216, 171,  -1,  -1,  -1, 208,  -1,  -1, 233,  -1,  -1,\n",
            "         -1,  -1, 162,  -1,  13,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 155,\n",
            "        207, 216,  -1,  -1,  -1, 216,  -1,  -1,  62,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1,  57])\n",
            "tensor([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1,  3, -1, -1, -1, -1, -1, -1, -1,  0, -1, -1,  3, -1, -1, -1,  4, -1,\n",
            "        -1, -1, -1, -1,  0, -1, -1, -1,  4, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1,  3, -1, -1,  4, -1, -1,  3, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1,  3, -1, -1, -1, -1, -1, -1, -1,  3, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1,  3, -1,  3, -1, -1, -1, -1,  0, -1, -1,  3,  3, -1, -1, -1,\n",
            "         4, -1, -1, -1, -1,  3, -1, -1, -1, -1, -1, -1, -1,  5, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1, -1, -1,  3, -1, -1, -1, -1,  3, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  4, -1, -1, -1, -1,  4,\n",
            "        -1, -1, -1, -1, -1, -1, -1,  4, -1, -1, -1,  4, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  3, -1,\n",
            "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
            "        -1,  4, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1, -1, -1,\n",
            "         3, -1, -1, -1])\n",
            "tensor([ -1,  -1, 181,  -1,  -1,  -1,  -1,  -1, 119,  -1, 137,  -1,  -1,  -1,\n",
            "         -1, 115,  -1,  -1,  -1, 102,  -1,  -1,  -1, 181,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  51,  -1,  -1,  -1,  22, 117,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 115,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  69,  51,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 197,\n",
            "         -1,  -1,  -1,  -1, 127,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 259,\n",
            "         -1,  -1,  -1,  -1,  -1,  36,  -1,  -1, 240,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1, 127,  36, 119,  -1,  -1, 155,  36,  -1,  -1, 200,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "        119,  -1,  -1,  -1,  -1,  -1,  -1,  -1, 119,  -1, 196,  -1,  -1, 119,\n",
            "         -1,  -1,  30,  -1, 205,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  84,\n",
            "         -1,  -1,  -1,  -1,  -1, 217,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  51,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1, 119,  -1,  -1,  -1, 183,  -1,  -1,  -1,  -1,  -1,  29, 216,  -1,\n",
            "         -1,  -1,  -1,  -1,  -1,  -1,  -1, 119,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1,  -1,  -1, 127, 102,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "         -1, 102,  -1, 119,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  51,  -1,  -1,\n",
            "        209,  -1,  -1,  -1,  -1, 188,  -1,  -1,  -1,  -1,  -1,  -1,  -1,  -1,\n",
            "        119,  -1,  -1,  -1])\n",
            "tensor([3, 4, 4, 5, 5, 3, 3, 4, 7, 6, 5, 6, 5, 5, 4, 6, 6, 5, 6, 5, 3, 3, 2, 6,\n",
            "        6, 6, 4, 0, 6, 5, 6, 6, 5, 5, 6, 4, 4, 5, 3, 8, 4, 3, 5, 5, 5, 6, 7, 5,\n",
            "        6, 5, 5, 4, 5, 6, 6, 8, 6, 5, 3, 5, 6, 4, 6, 5, 6, 2, 3, 7, 9, 7, 5, 7,\n",
            "        4, 2, 5, 4, 6, 7, 6, 5, 7, 6, 5, 6, 6, 4, 6, 4, 5, 4, 6, 8, 4, 4, 6, 4,\n",
            "        5, 5, 7, 5, 5, 5, 5, 8, 3, 4, 6, 3, 4, 5, 7, 2, 5, 4, 6, 8, 4, 6, 2, 5,\n",
            "        5, 5, 8, 6, 6, 7, 4, 4, 4, 4, 5, 1, 4, 3, 6, 6, 5, 6, 5, 7, 6, 6, 4, 7,\n",
            "        4, 4, 7, 6, 5, 6, 5, 5, 5, 5, 6, 4, 2, 0, 5, 8, 3, 5, 4, 5, 5, 9, 5, 4,\n",
            "        4, 3, 5, 5, 6, 4, 5, 8, 6, 5, 5, 3, 2, 4, 5, 5, 6, 5, 7, 4, 4, 4, 6, 6,\n",
            "        1, 7, 4, 5, 5, 5, 5, 5, 7, 5, 7, 6, 4, 8, 5, 5, 5, 5, 6, 6, 7, 6, 6, 5,\n",
            "        5, 1, 8, 5, 4, 5, 6, 7, 2, 6, 5, 5, 4, 5, 8, 1, 5, 3, 5, 2, 4, 5, 6, 5,\n",
            "        6, 6, 6, 5, 7, 5, 6, 4, 4, 4, 6, 4, 6, 5, 2, 3])\n",
            "tensor([ 6,  3,  4,  4,  4,  0,  7,  4,  5,  1,  6,  7,  6,  5,  2,  4,  4,  4,\n",
            "         2,  6,  3,  1,  3,  5,  7,  6,  9,  0,  3,  6,  5,  7,  5,  6,  5,  5,\n",
            "         4,  8,  8,  5,  5,  7,  5,  5,  4,  3,  7,  4,  4,  6,  6,  4,  4,  3,\n",
            "         4,  7,  3,  6,  6,  3,  5,  6,  5,  8,  5,  6,  7,  8,  4,  7,  2,  6,\n",
            "         7,  4,  2,  4,  6,  6,  7,  5,  4,  2,  5,  6,  7,  7,  0,  5,  5,  7,\n",
            "         8,  7,  3,  7,  7,  6,  5,  5,  4,  4,  3,  4,  8,  6,  5,  5,  4,  6,\n",
            "         0,  5,  3,  2,  4,  6,  9,  5,  6,  3,  4,  8,  1,  6,  5,  6,  9,  4,\n",
            "         9,  5,  3,  5,  5,  2,  5,  6,  4,  6,  6,  7,  3,  6,  8,  6,  5,  8,\n",
            "         5,  5,  6,  4,  5,  5,  5,  5,  5,  4,  5,  3,  3,  1,  5,  5,  2,  5,\n",
            "         3,  7,  6,  5,  8,  6,  6,  0,  2,  8,  4,  6,  3,  3,  4,  8,  6,  0,\n",
            "         5,  3,  6,  6,  4,  4,  6,  5,  5,  8,  9,  5,  0,  5,  6,  2,  5,  5,\n",
            "         5,  5,  6,  3,  5,  6,  6,  6,  4,  5,  6,  3,  9,  2,  5,  5,  6,  4,\n",
            "         3,  5,  3,  3,  3, 10,  7,  6,  3,  6,  2,  5,  4,  6,  8,  6,  4,  4,\n",
            "         5,  6,  3,  5,  7,  7,  3,  7,  8,  4,  7,  7,  3,  7,  5,  4,  5,  7,\n",
            "         5, 10,  3,  4])\n",
            "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "class EmoSet(Dataset):\n",
        "    ATTRIBUTES_MULTI_CLASS = [\n",
        "        'scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness',\n",
        "    ]\n",
        "    ATTRIBUTES_MULTI_LABEL = [\n",
        "        'object'\n",
        "    ]\n",
        "    NUM_CLASSES = {\n",
        "        'brightness': 11,\n",
        "        'colorfulness': 11,\n",
        "        'scene': 254,\n",
        "        'object': 409,\n",
        "        'facial_expression': 6,\n",
        "        'human_action': 264,\n",
        "    }\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 num_emotion_classes,\n",
        "                 phase,\n",
        "                 ):\n",
        "        assert num_emotion_classes in (8, 2)\n",
        "        assert phase in ('train', 'val', 'test')\n",
        "        self.transforms_dict = self.get_data_transforms()\n",
        "        self.info = self.get_info(data_root, num_emotion_classes)\n",
        "        if phase == 'train':\n",
        "            self.transform = self.transforms_dict['train']\n",
        "        elif phase == 'val':\n",
        "            self.transform = self.transforms_dict['val']\n",
        "        elif phase == 'test':\n",
        "            self.transform = self.transforms_dict['test']\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        data_store = json.load(open(os.path.join(data_root, f'{phase}.json')))\n",
        "        self.data_store = [\n",
        "            [\n",
        "                self.info['emotion']['label2idx'][item[0]],\n",
        "                os.path.join(data_root, item[1]),\n",
        "                os.path.join(data_root, item[2])\n",
        "            ]\n",
        "            for item in data_store\n",
        "        ]\n",
        "    @classmethod\n",
        "    def get_data_transforms(cls):\n",
        "        transforms_dict = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(224),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'test': transforms.Compose([\n",
        "                transforms.Resize(224),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "        }\n",
        "        return transforms_dict\n",
        "    def get_info(self, data_root, num_emotion_classes):\n",
        "        assert num_emotion_classes in (8, 2)\n",
        "        info = json.load(open(os.path.join(data_root, 'info.json')))\n",
        "        if num_emotion_classes == 8:\n",
        "            pass\n",
        "        elif num_emotion_classes == 2:\n",
        "            emotion_info = {\n",
        "                'label2idx': {\n",
        "                    'amusement': 0,\n",
        "                    'awe': 0,\n",
        "                    'contentment': 0,\n",
        "                    'excitement': 0,\n",
        "                    'anger': 1,\n",
        "                    'disgust': 1,\n",
        "                    'fear': 1,\n",
        "                    'sadness': 1,\n",
        "                },\n",
        "                'idx2label': {\n",
        "                    '0': 'positive',\n",
        "                    '1': 'negative',\n",
        "                }\n",
        "            }\n",
        "            info['emotion'] = emotion_info\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return info\n",
        "    def load_image_by_path(self, path):\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "    def load_annotation_by_path(self, path):\n",
        "        json_data = json.load(open(path))\n",
        "        return json_data\n",
        "    def __getitem__(self, item):\n",
        "        emotion_label_idx, image_path, annotation_path = self.data_store[item]\n",
        "        image = self.load_image_by_path(image_path)\n",
        "        annotation_data = self.load_annotation_by_path(annotation_path)\n",
        "        data = { 'image': image, 'emotion_label_idx': emotion_label_idx}\n",
        "        for attribute in self.ATTRIBUTES_MULTI_CLASS:\n",
        "            attribute_label_idx = -1\n",
        "            if attribute in annotation_data:\n",
        "                attribute_label_idx = self.info[attribute]['label2idx'][str(annotation_data[attribute])]\n",
        "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
        "        for attribute in self.ATTRIBUTES_MULTI_LABEL:\n",
        "            assert attribute == 'object'\n",
        "            num_classes = self.NUM_CLASSES[attribute]\n",
        "            attribute_label_idx = torch.zeros(num_classes)\n",
        "            if attribute in annotation_data:\n",
        "                for label in annotation_data[attribute]:\n",
        "                    attribute_label_idx[self.info[attribute]['label2idx'][label]] = 1\n",
        "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
        "        return data\n",
        "    def __len__(self):\n",
        "        return len(self.data_store)\n",
        "if __name__ == '__main__':\n",
        "    data_root = \"C:/Users/HP/Downloads/Emotion-Analysis/dataset\"\n",
        "    num_emotion_classes = 8\n",
        "    phase = 'train'\n",
        "    dataset = EmoSet(\n",
        "        data_root=data_root,\n",
        "        num_emotion_classes=num_emotion_classes,\n",
        "        phase=phase,\n",
        "    )\n",
        "    dataloader = DataLoader(dataset, batch_size = 256, shuffle = True)\n",
        "    print(dataloader)\n",
        "    for i,data in enumerate(dataloader):\n",
        "        print(data['emotion_label_idx'])\n",
        "        print(data['scene_label_idx'])\n",
        "        print(data['facial_expression_label_idx'])\n",
        "        print(data['human_action_label_idx'])\n",
        "        print(data['brightness_label_idx'])\n",
        "        print(data['colorfulness_label_idx'])\n",
        "        print(data['object_label_idx'])\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='train')\n",
        "valid_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='val')\n",
        "test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "class AttributePredictor(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AttributePredictor, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "def train_attribute_model(train_dataset, valid_dataset, num_classes, attribute_name, epochs=1):\n",
        "    train_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "    model = AttributePredictor(num_classes)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    best_valid_acc = 0.0\n",
        "    best_model_wts = model.state_dict()\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "        i=0\n",
        "        for batch in train_loader:\n",
        "            i+=1\n",
        "            images = batch['image']\n",
        "            labels = batch[f\"{attribute_name}_label_idx\"]\n",
        "            valid_indices = labels >= 0\n",
        "            images = images[valid_indices]\n",
        "            labels = labels[valid_indices]\n",
        "            if len(labels) == 0:\n",
        "                continue\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "            if i % 20 == 0:\n",
        "                    if(total==0):\n",
        "                        total=1\n",
        "                    batch_accuracy = 100 * correct / total\n",
        "                    print(f\"Batch {i}, Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
        "        if(total==0):\n",
        "            total=1\n",
        "        train_acc = 100 * correct / total\n",
        "        print(f\"Epoch {epoch+1}: Train Loss: {total_loss/len(train_loader):.4f}, Train Acc: {train_acc:.2f}%\")\n",
        "        model.eval()\n",
        "        correct, total = 0, 0\n",
        "        with torch.no_grad():\n",
        "            for batch in valid_loader:\n",
        "                valid_indices = labels >= 0\n",
        "                images = images[valid_indices]\n",
        "                labels = labels[valid_indices]\n",
        "                if len(labels) == 0:\n",
        "                    continue\n",
        "                outputs = model(images)\n",
        "                correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "        if(total==0):\n",
        "            total=1\n",
        "        valid_acc = 100 * correct / total\n",
        "        print(f\"Validation Accuracy: {valid_acc:.2f}%\")\n",
        "        if valid_acc > best_valid_acc:\n",
        "            best_valid_acc = valid_acc\n",
        "            best_model_wts = model.state_dict().copy()\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    print(f\"Best Validation Accuracy for {attribute_name}: {best_valid_acc:.2f}%\")\n",
        "    torch.save(model.state_dict(), f\"{attribute_name}_predictor.pth\")\n",
        "    print(f\"Model for {attribute_name} saved!\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model for scene...\n",
            "Epoch 1\n",
            "Batch 20, Loss: 5.1681, Accuracy: 19.01%\n",
            "Batch 40, Loss: 4.4692, Accuracy: 16.09%\n",
            "Batch 60, Loss: 2.1848, Accuracy: 14.85%\n",
            "Batch 80, Loss: 4.4012, Accuracy: 16.39%\n",
            "Batch 100, Loss: 4.1697, Accuracy: 15.94%\n",
            "Batch 120, Loss: 4.3619, Accuracy: 15.75%\n",
            "Batch 140, Loss: 4.2890, Accuracy: 15.26%\n",
            "Batch 160, Loss: 6.9186, Accuracy: 14.97%\n",
            "Batch 180, Loss: 3.7724, Accuracy: 14.83%\n",
            "Batch 200, Loss: 3.7108, Accuracy: 15.03%\n",
            "Batch 220, Loss: 5.6682, Accuracy: 14.41%\n",
            "Batch 240, Loss: 4.8617, Accuracy: 14.14%\n",
            "Batch 260, Loss: 4.4241, Accuracy: 14.20%\n",
            "Batch 280, Loss: 4.8973, Accuracy: 14.85%\n",
            "Batch 300, Loss: 5.8158, Accuracy: 14.98%\n",
            "Batch 320, Loss: 3.0999, Accuracy: 14.94%\n",
            "Batch 340, Loss: 4.5383, Accuracy: 15.17%\n",
            "Batch 360, Loss: 4.6202, Accuracy: 15.51%\n",
            "Batch 380, Loss: 3.3664, Accuracy: 15.20%\n",
            "Batch 400, Loss: 5.7025, Accuracy: 15.06%\n",
            "Batch 420, Loss: 5.1143, Accuracy: 14.90%\n",
            "Batch 440, Loss: 3.7598, Accuracy: 15.26%\n",
            "Batch 460, Loss: 4.3684, Accuracy: 15.42%\n",
            "Batch 480, Loss: 3.8707, Accuracy: 15.44%\n",
            "Batch 500, Loss: 4.8461, Accuracy: 15.74%\n",
            "Batch 520, Loss: 3.6539, Accuracy: 15.55%\n",
            "Batch 540, Loss: 3.2044, Accuracy: 15.36%\n",
            "Epoch 1: Train Loss: 4.2241, Train Acc: 15.30%\n",
            "Validation Accuracy: 0.00%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 4.1870, Accuracy: 14.38%\n",
            "Batch 40, Loss: 3.6485, Accuracy: 16.61%\n",
            "Batch 60, Loss: 4.3613, Accuracy: 15.67%\n",
            "Batch 80, Loss: 5.2184, Accuracy: 15.98%\n",
            "Batch 100, Loss: 3.6174, Accuracy: 16.51%\n",
            "Batch 120, Loss: 4.4748, Accuracy: 17.10%\n",
            "Batch 140, Loss: 3.3490, Accuracy: 17.19%\n",
            "Batch 160, Loss: 3.9177, Accuracy: 17.58%\n",
            "Batch 180, Loss: 3.0901, Accuracy: 17.77%\n",
            "Batch 200, Loss: 3.4958, Accuracy: 18.24%\n",
            "Batch 220, Loss: 3.8482, Accuracy: 17.55%\n",
            "Batch 240, Loss: 3.6020, Accuracy: 18.29%\n",
            "Batch 260, Loss: 3.2954, Accuracy: 18.27%\n",
            "Batch 280, Loss: 4.1715, Accuracy: 18.37%\n",
            "Batch 300, Loss: 4.0463, Accuracy: 18.30%\n",
            "Batch 320, Loss: 4.1999, Accuracy: 17.96%\n",
            "Batch 340, Loss: 3.9025, Accuracy: 17.86%\n",
            "Batch 360, Loss: 2.9331, Accuracy: 18.15%\n",
            "Batch 380, Loss: 4.5426, Accuracy: 18.17%\n",
            "Batch 400, Loss: 3.3886, Accuracy: 18.31%\n",
            "Batch 420, Loss: 3.4636, Accuracy: 18.22%\n",
            "Batch 440, Loss: 3.7341, Accuracy: 18.31%\n",
            "Batch 460, Loss: 4.1064, Accuracy: 18.42%\n",
            "Batch 480, Loss: 4.1679, Accuracy: 18.62%\n",
            "Batch 500, Loss: 2.8262, Accuracy: 18.83%\n",
            "Batch 520, Loss: 3.3379, Accuracy: 18.99%\n",
            "Batch 540, Loss: 3.7484, Accuracy: 19.12%\n",
            "Epoch 2: Train Loss: 3.8151, Train Acc: 19.00%\n",
            "Validation Accuracy: 37.50%\n",
            "Epoch 3\n",
            "Batch 20, Loss: 3.4806, Accuracy: 23.74%\n",
            "Batch 40, Loss: 4.0618, Accuracy: 23.49%\n",
            "Batch 60, Loss: 2.8134, Accuracy: 24.37%\n",
            "Batch 80, Loss: 3.6753, Accuracy: 22.93%\n",
            "Batch 100, Loss: 3.5664, Accuracy: 22.27%\n",
            "Batch 120, Loss: 2.1344, Accuracy: 22.57%\n",
            "Batch 140, Loss: 3.7020, Accuracy: 22.40%\n",
            "Batch 160, Loss: 5.1439, Accuracy: 22.25%\n",
            "Batch 180, Loss: 3.3453, Accuracy: 21.90%\n",
            "Batch 200, Loss: 3.6652, Accuracy: 21.84%\n",
            "Batch 220, Loss: 3.7835, Accuracy: 21.72%\n",
            "Batch 240, Loss: 3.1644, Accuracy: 22.04%\n",
            "Batch 260, Loss: 3.7226, Accuracy: 21.88%\n",
            "Batch 280, Loss: 3.7951, Accuracy: 22.07%\n",
            "Batch 300, Loss: 3.8429, Accuracy: 22.04%\n",
            "Batch 320, Loss: 3.1051, Accuracy: 21.94%\n",
            "Batch 340, Loss: 3.4597, Accuracy: 22.34%\n",
            "Batch 360, Loss: 4.5360, Accuracy: 22.26%\n",
            "Batch 380, Loss: 4.2755, Accuracy: 22.75%\n",
            "Batch 400, Loss: 3.1115, Accuracy: 22.54%\n",
            "Batch 420, Loss: 3.0130, Accuracy: 22.73%\n",
            "Batch 440, Loss: 3.5071, Accuracy: 22.84%\n",
            "Batch 460, Loss: 3.2287, Accuracy: 22.99%\n",
            "Batch 480, Loss: 3.1730, Accuracy: 23.11%\n",
            "Batch 500, Loss: 5.3839, Accuracy: 23.12%\n",
            "Batch 520, Loss: 3.6447, Accuracy: 23.04%\n",
            "Batch 540, Loss: 3.1689, Accuracy: 23.05%\n",
            "Epoch 3: Train Loss: 3.5192, Train Acc: 23.06%\n",
            "Validation Accuracy: 42.86%\n",
            "Epoch 4\n",
            "Batch 20, Loss: 2.9571, Accuracy: 29.49%\n",
            "Batch 40, Loss: 2.7149, Accuracy: 23.03%\n",
            "Batch 60, Loss: 4.8205, Accuracy: 23.16%\n",
            "Batch 80, Loss: 3.9319, Accuracy: 23.31%\n",
            "Batch 100, Loss: 5.0328, Accuracy: 24.23%\n",
            "Batch 120, Loss: 3.0800, Accuracy: 25.87%\n",
            "Batch 140, Loss: 3.0419, Accuracy: 25.78%\n",
            "Batch 160, Loss: 4.2071, Accuracy: 25.66%\n",
            "Batch 180, Loss: 3.9631, Accuracy: 25.38%\n",
            "Batch 200, Loss: 2.9484, Accuracy: 26.06%\n",
            "Batch 220, Loss: 2.9422, Accuracy: 25.54%\n",
            "Batch 240, Loss: 2.6845, Accuracy: 25.48%\n",
            "Batch 260, Loss: 1.9586, Accuracy: 25.65%\n",
            "Batch 280, Loss: 4.1054, Accuracy: 25.92%\n",
            "Batch 300, Loss: 3.6643, Accuracy: 26.23%\n",
            "Batch 320, Loss: 2.4713, Accuracy: 26.58%\n",
            "Batch 340, Loss: 2.8877, Accuracy: 26.27%\n",
            "Batch 360, Loss: 4.1502, Accuracy: 26.05%\n",
            "Batch 380, Loss: 2.1369, Accuracy: 26.28%\n",
            "Batch 400, Loss: 3.0768, Accuracy: 26.67%\n",
            "Batch 420, Loss: 4.4996, Accuracy: 26.45%\n",
            "Batch 440, Loss: 3.7381, Accuracy: 26.44%\n",
            "Batch 460, Loss: 3.8778, Accuracy: 26.47%\n",
            "Batch 480, Loss: 2.6401, Accuracy: 26.45%\n",
            "Batch 500, Loss: 2.5688, Accuracy: 26.64%\n",
            "Batch 520, Loss: 3.1924, Accuracy: 26.94%\n",
            "Batch 540, Loss: 3.1847, Accuracy: 27.25%\n",
            "Epoch 4: Train Loss: 3.3062, Train Acc: 27.31%\n",
            "Validation Accuracy: 28.57%\n",
            "Epoch 5\n",
            "Batch 20, Loss: 3.0154, Accuracy: 26.67%\n",
            "Batch 40, Loss: 4.6516, Accuracy: 27.18%\n",
            "Batch 60, Loss: 2.3602, Accuracy: 27.79%\n",
            "Batch 80, Loss: 4.2552, Accuracy: 26.78%\n",
            "Batch 100, Loss: 2.6516, Accuracy: 26.05%\n",
            "Batch 120, Loss: 2.3846, Accuracy: 25.89%\n",
            "Batch 140, Loss: 2.7998, Accuracy: 26.72%\n",
            "Batch 160, Loss: 2.1236, Accuracy: 27.50%\n",
            "Batch 180, Loss: 3.4252, Accuracy: 27.55%\n",
            "Batch 200, Loss: 3.5885, Accuracy: 27.79%\n",
            "Batch 220, Loss: 4.5326, Accuracy: 28.28%\n",
            "Batch 240, Loss: 3.9138, Accuracy: 28.27%\n",
            "Batch 260, Loss: 2.6608, Accuracy: 28.05%\n",
            "Batch 280, Loss: 3.9704, Accuracy: 28.16%\n",
            "Batch 300, Loss: 2.0785, Accuracy: 28.13%\n",
            "Batch 320, Loss: 3.8711, Accuracy: 28.21%\n",
            "Batch 340, Loss: 2.6756, Accuracy: 28.18%\n",
            "Batch 360, Loss: 3.2074, Accuracy: 28.29%\n",
            "Batch 380, Loss: 2.2889, Accuracy: 28.36%\n",
            "Batch 400, Loss: 3.8928, Accuracy: 28.82%\n",
            "Batch 420, Loss: 3.2545, Accuracy: 28.86%\n",
            "Batch 440, Loss: 5.4460, Accuracy: 28.82%\n",
            "Batch 460, Loss: 4.5421, Accuracy: 28.84%\n",
            "Batch 480, Loss: 2.4370, Accuracy: 28.77%\n",
            "Batch 500, Loss: 4.6511, Accuracy: 28.67%\n",
            "Batch 520, Loss: 3.0777, Accuracy: 28.80%\n",
            "Batch 540, Loss: 3.0177, Accuracy: 28.85%\n",
            "Epoch 5: Train Loss: 3.1916, Train Acc: 28.78%\n",
            "Validation Accuracy: 33.33%\n",
            "Best Validation Accuracy for scene: 42.86%\n",
            "Model for scene saved!\n",
            "Training model for facial_expression...\n",
            "Epoch 1\n",
            "Batch 20, Loss: 3.0148, Accuracy: 58.59%\n",
            "Batch 40, Loss: 0.8893, Accuracy: 61.29%\n",
            "Batch 60, Loss: 0.2853, Accuracy: 65.08%\n",
            "Batch 80, Loss: 0.4348, Accuracy: 65.52%\n",
            "Batch 100, Loss: 1.2921, Accuracy: 68.50%\n",
            "Batch 120, Loss: 0.1503, Accuracy: 69.82%\n",
            "Batch 140, Loss: 1.0390, Accuracy: 70.96%\n",
            "Batch 160, Loss: 0.8109, Accuracy: 71.37%\n",
            "Batch 180, Loss: 0.8303, Accuracy: 70.95%\n",
            "Batch 200, Loss: 1.6442, Accuracy: 71.29%\n",
            "Batch 220, Loss: 0.8515, Accuracy: 71.63%\n",
            "Batch 240, Loss: 0.6142, Accuracy: 70.91%\n",
            "Batch 260, Loss: 1.8541, Accuracy: 71.27%\n",
            "Batch 280, Loss: 0.6477, Accuracy: 71.75%\n",
            "Batch 300, Loss: 1.2269, Accuracy: 71.97%\n",
            "Batch 320, Loss: 0.3390, Accuracy: 72.26%\n",
            "Batch 340, Loss: 1.0941, Accuracy: 72.16%\n",
            "Batch 360, Loss: 1.5890, Accuracy: 72.37%\n",
            "Batch 380, Loss: 0.7232, Accuracy: 72.38%\n",
            "Batch 400, Loss: 1.0909, Accuracy: 72.39%\n",
            "Batch 420, Loss: 2.4926, Accuracy: 72.69%\n",
            "Batch 440, Loss: 0.5988, Accuracy: 72.75%\n",
            "Batch 460, Loss: 1.3965, Accuracy: 72.40%\n",
            "Batch 480, Loss: 1.1054, Accuracy: 72.22%\n",
            "Batch 500, Loss: 0.2214, Accuracy: 72.64%\n",
            "Batch 520, Loss: 0.2993, Accuracy: 72.60%\n",
            "Batch 540, Loss: 0.5829, Accuracy: 72.87%\n",
            "Epoch 1: Train Loss: 0.9392, Train Acc: 72.90%\n",
            "Validation Accuracy: 80.00%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 0.7788, Accuracy: 74.42%\n",
            "Batch 40, Loss: 1.6267, Accuracy: 72.63%\n",
            "Batch 60, Loss: 2.0164, Accuracy: 73.93%\n",
            "Batch 80, Loss: 0.3276, Accuracy: 75.47%\n",
            "Batch 100, Loss: 1.9124, Accuracy: 74.42%\n",
            "Batch 120, Loss: 1.1791, Accuracy: 74.16%\n",
            "Batch 140, Loss: 1.1030, Accuracy: 73.80%\n",
            "Batch 160, Loss: 1.7238, Accuracy: 74.01%\n",
            "Batch 180, Loss: 0.2548, Accuracy: 73.91%\n",
            "Batch 200, Loss: 0.3080, Accuracy: 74.19%\n",
            "Batch 220, Loss: 0.1893, Accuracy: 74.26%\n",
            "Batch 240, Loss: 0.0598, Accuracy: 74.71%\n",
            "Batch 260, Loss: 0.9398, Accuracy: 74.51%\n",
            "Batch 280, Loss: 0.9091, Accuracy: 73.99%\n",
            "Batch 300, Loss: 0.3114, Accuracy: 74.13%\n",
            "Batch 320, Loss: 0.3097, Accuracy: 73.95%\n",
            "Batch 340, Loss: 0.2346, Accuracy: 73.86%\n",
            "Batch 360, Loss: 3.7421, Accuracy: 73.79%\n",
            "Batch 380, Loss: 0.3072, Accuracy: 74.07%\n",
            "Batch 400, Loss: 0.2045, Accuracy: 74.25%\n",
            "Batch 420, Loss: 1.8010, Accuracy: 74.16%\n",
            "Batch 440, Loss: 0.3906, Accuracy: 74.16%\n",
            "Batch 460, Loss: 0.4784, Accuracy: 74.31%\n",
            "Batch 480, Loss: 0.3557, Accuracy: 74.18%\n",
            "Batch 500, Loss: 0.3154, Accuracy: 74.43%\n",
            "Batch 520, Loss: 2.1293, Accuracy: 74.43%\n",
            "Batch 540, Loss: 1.2980, Accuracy: 74.42%\n",
            "Epoch 2: Train Loss: 0.8803, Train Acc: 74.33%\n",
            "Validation Accuracy: 66.67%\n",
            "Epoch 3\n",
            "Batch 20, Loss: 1.1602, Accuracy: 82.73%\n",
            "Batch 40, Loss: 0.5192, Accuracy: 78.24%\n",
            "Batch 60, Loss: 0.0819, Accuracy: 77.40%\n",
            "Batch 80, Loss: 0.8721, Accuracy: 76.35%\n",
            "Batch 100, Loss: 1.1025, Accuracy: 77.11%\n",
            "Batch 120, Loss: 0.8740, Accuracy: 75.80%\n",
            "Batch 140, Loss: 0.6335, Accuracy: 73.76%\n",
            "Batch 160, Loss: 1.1425, Accuracy: 73.19%\n",
            "Batch 180, Loss: 1.1406, Accuracy: 73.03%\n",
            "Batch 200, Loss: 0.1864, Accuracy: 74.04%\n",
            "Batch 220, Loss: 0.2521, Accuracy: 74.65%\n",
            "Batch 240, Loss: 0.2987, Accuracy: 75.13%\n",
            "Batch 260, Loss: 0.6756, Accuracy: 75.06%\n",
            "Batch 280, Loss: 1.9107, Accuracy: 74.78%\n",
            "Batch 300, Loss: 0.3057, Accuracy: 74.77%\n",
            "Batch 320, Loss: 3.5295, Accuracy: 73.85%\n",
            "Batch 340, Loss: 0.6930, Accuracy: 73.52%\n",
            "Batch 360, Loss: 0.9093, Accuracy: 73.62%\n",
            "Batch 380, Loss: 0.7806, Accuracy: 73.70%\n",
            "Batch 400, Loss: 0.6014, Accuracy: 73.79%\n",
            "Batch 420, Loss: 0.7851, Accuracy: 73.32%\n",
            "Batch 440, Loss: 0.3291, Accuracy: 73.33%\n",
            "Batch 460, Loss: 2.9581, Accuracy: 73.13%\n",
            "Batch 480, Loss: 1.0278, Accuracy: 73.03%\n",
            "Batch 500, Loss: 0.9565, Accuracy: 73.03%\n",
            "Batch 520, Loss: 0.5131, Accuracy: 72.85%\n",
            "Batch 540, Loss: 1.1158, Accuracy: 73.03%\n",
            "Epoch 3: Train Loss: 0.9119, Train Acc: 73.20%\n"
          ]
        },
        {
          "ename": "ZeroDivisionError",
          "evalue": "division by zero",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[35], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, num_classes \u001b[38;5;129;01min\u001b[39;00m attribute_classes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m     trained_models[attr] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attribute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[33], line 70\u001b[0m, in \u001b[0;36mtrain_attribute_model\u001b[1;34m(train_dataset, valid_dataset, num_classes, attribute_name, epochs)\u001b[0m\n\u001b[0;32m     67\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m labels)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     68\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m valid_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m100\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcorrect\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid_acc \u001b[38;5;241m>\u001b[39m best_valid_acc:\n",
            "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "attribute_classes = {\n",
        "    'scene': 254,\n",
        "    'facial_expression': 6,\n",
        "    'human_action': 264,\n",
        "    'brightness': 11,\n",
        "    'colorfulness': 11,\n",
        "    'object': 409\n",
        "}\n",
        "trained_models = {}\n",
        "for attr, num_classes in attribute_classes.items():\n",
        "    print(f\"Training model for {attr}...\")\n",
        "    trained_models[attr] = train_attribute_model(train_dataset, valid_dataset, num_classes, attr,  epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model for human_action...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Batch 20, Loss: 6.6359, Accuracy: 10.34%\n",
            "Batch 40, Loss: 4.9701, Accuracy: 10.45%\n",
            "Batch 60, Loss: 4.0024, Accuracy: 10.85%\n",
            "Batch 80, Loss: 5.3674, Accuracy: 13.38%\n",
            "Batch 100, Loss: 5.0429, Accuracy: 14.38%\n",
            "Batch 120, Loss: 3.3590, Accuracy: 14.24%\n",
            "Batch 140, Loss: 4.6282, Accuracy: 13.81%\n",
            "Batch 160, Loss: 3.2609, Accuracy: 13.35%\n",
            "Batch 180, Loss: 4.7377, Accuracy: 13.32%\n",
            "Batch 200, Loss: 3.6289, Accuracy: 13.51%\n",
            "Batch 220, Loss: 5.8539, Accuracy: 13.33%\n",
            "Batch 240, Loss: 3.2147, Accuracy: 13.09%\n",
            "Batch 260, Loss: 5.2700, Accuracy: 12.94%\n",
            "Batch 280, Loss: 3.1328, Accuracy: 13.33%\n",
            "Batch 300, Loss: 3.7282, Accuracy: 12.88%\n",
            "Batch 320, Loss: 3.5053, Accuracy: 13.06%\n",
            "Batch 340, Loss: 3.9986, Accuracy: 13.10%\n",
            "Batch 360, Loss: 3.2852, Accuracy: 13.53%\n",
            "Batch 380, Loss: 3.5696, Accuracy: 13.55%\n",
            "Batch 400, Loss: 5.8132, Accuracy: 13.98%\n",
            "Batch 420, Loss: 3.0795, Accuracy: 14.03%\n",
            "Batch 440, Loss: 4.5378, Accuracy: 14.10%\n",
            "Batch 460, Loss: 5.1289, Accuracy: 14.18%\n",
            "Batch 480, Loss: 3.3228, Accuracy: 14.34%\n",
            "Batch 500, Loss: 4.5570, Accuracy: 14.13%\n",
            "Batch 520, Loss: 5.1885, Accuracy: 14.14%\n",
            "Batch 540, Loss: 3.3167, Accuracy: 14.25%\n",
            "Epoch 1: Train Loss: 4.1312, Train Acc: 14.25%\n",
            "Validation Accuracy: 0.00%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 2.8766, Accuracy: 18.89%\n",
            "Batch 40, Loss: 2.7398, Accuracy: 18.62%\n",
            "Batch 60, Loss: 4.1811, Accuracy: 16.67%\n",
            "Batch 80, Loss: 3.8762, Accuracy: 14.48%\n",
            "Batch 100, Loss: 3.4043, Accuracy: 14.19%\n",
            "Batch 120, Loss: 4.2391, Accuracy: 14.73%\n",
            "Batch 140, Loss: 3.0023, Accuracy: 15.07%\n",
            "Batch 160, Loss: 3.6993, Accuracy: 15.87%\n",
            "Batch 180, Loss: 3.4470, Accuracy: 15.86%\n",
            "Batch 200, Loss: 3.1144, Accuracy: 17.31%\n",
            "Batch 220, Loss: 4.7695, Accuracy: 17.73%\n",
            "Batch 240, Loss: 3.6758, Accuracy: 17.82%\n",
            "Batch 260, Loss: 2.8452, Accuracy: 17.70%\n",
            "Batch 280, Loss: 3.7554, Accuracy: 17.56%\n",
            "Batch 300, Loss: 4.4520, Accuracy: 17.89%\n",
            "Batch 320, Loss: 4.3235, Accuracy: 17.88%\n",
            "Batch 340, Loss: 4.6233, Accuracy: 17.86%\n",
            "Batch 360, Loss: 4.6877, Accuracy: 18.09%\n",
            "Batch 380, Loss: 2.8466, Accuracy: 17.80%\n",
            "Batch 400, Loss: 4.9472, Accuracy: 17.49%\n",
            "Batch 420, Loss: 3.4434, Accuracy: 17.46%\n",
            "Batch 440, Loss: 5.2895, Accuracy: 17.27%\n",
            "Batch 460, Loss: 3.9588, Accuracy: 17.29%\n",
            "Batch 480, Loss: 4.3622, Accuracy: 17.06%\n",
            "Batch 500, Loss: 3.1059, Accuracy: 16.91%\n",
            "Batch 520, Loss: 3.6029, Accuracy: 16.90%\n",
            "Batch 540, Loss: 3.5529, Accuracy: 16.71%\n",
            "Epoch 2: Train Loss: 3.8010, Train Acc: 16.75%\n",
            "Validation Accuracy: 33.33%\n",
            "Epoch 3\n",
            "Batch 20, Loss: 2.7414, Accuracy: 14.89%\n",
            "Batch 40, Loss: 4.5691, Accuracy: 13.64%\n",
            "Batch 60, Loss: 2.7905, Accuracy: 13.84%\n",
            "Batch 80, Loss: 3.6757, Accuracy: 14.32%\n",
            "Batch 100, Loss: 3.8063, Accuracy: 14.55%\n",
            "Batch 120, Loss: 3.0742, Accuracy: 13.70%\n",
            "Batch 140, Loss: 3.8578, Accuracy: 14.78%\n",
            "Batch 160, Loss: 3.8707, Accuracy: 16.45%\n",
            "Batch 180, Loss: 3.4302, Accuracy: 16.14%\n",
            "Batch 200, Loss: 1.6899, Accuracy: 15.79%\n",
            "Batch 220, Loss: 3.1339, Accuracy: 15.38%\n",
            "Batch 240, Loss: 4.6041, Accuracy: 15.46%\n",
            "Batch 260, Loss: 3.1482, Accuracy: 15.37%\n",
            "Batch 280, Loss: 3.2605, Accuracy: 15.38%\n",
            "Batch 300, Loss: 3.3628, Accuracy: 15.62%\n",
            "Batch 320, Loss: 3.7203, Accuracy: 15.75%\n",
            "Batch 340, Loss: 1.6348, Accuracy: 16.08%\n",
            "Batch 360, Loss: 3.1540, Accuracy: 16.23%\n",
            "Batch 380, Loss: 3.5762, Accuracy: 16.38%\n",
            "Batch 400, Loss: 2.7184, Accuracy: 16.33%\n",
            "Batch 420, Loss: 3.5673, Accuracy: 16.97%\n",
            "Batch 440, Loss: 3.7833, Accuracy: 17.17%\n",
            "Batch 460, Loss: 2.6065, Accuracy: 17.36%\n",
            "Batch 480, Loss: 4.3402, Accuracy: 17.64%\n",
            "Batch 500, Loss: 4.6881, Accuracy: 17.41%\n",
            "Batch 520, Loss: 3.1077, Accuracy: 17.69%\n",
            "Batch 540, Loss: 3.6003, Accuracy: 17.92%\n",
            "Epoch 3: Train Loss: 3.6949, Train Acc: 18.01%\n",
            "Validation Accuracy: 66.67%\n",
            "Epoch 4\n",
            "Batch 20, Loss: 3.0227, Accuracy: 26.36%\n",
            "Batch 40, Loss: 4.7647, Accuracy: 23.15%\n",
            "Batch 60, Loss: 2.9957, Accuracy: 20.97%\n",
            "Batch 80, Loss: 3.7126, Accuracy: 21.79%\n",
            "Batch 100, Loss: 1.9498, Accuracy: 21.53%\n",
            "Batch 120, Loss: 3.2034, Accuracy: 20.47%\n",
            "Batch 140, Loss: 3.9196, Accuracy: 20.21%\n",
            "Batch 160, Loss: 3.0172, Accuracy: 20.60%\n",
            "Batch 180, Loss: 4.3326, Accuracy: 21.11%\n",
            "Batch 200, Loss: 3.9009, Accuracy: 20.92%\n",
            "Batch 220, Loss: 3.6961, Accuracy: 20.29%\n",
            "Batch 240, Loss: 2.4664, Accuracy: 20.32%\n",
            "Batch 260, Loss: 4.1524, Accuracy: 19.95%\n",
            "Batch 280, Loss: 3.2822, Accuracy: 19.81%\n",
            "Batch 300, Loss: 3.0802, Accuracy: 19.45%\n",
            "Batch 320, Loss: 4.0111, Accuracy: 19.95%\n",
            "Batch 340, Loss: 4.7792, Accuracy: 19.96%\n",
            "Batch 360, Loss: 3.6286, Accuracy: 19.79%\n",
            "Batch 380, Loss: 3.6915, Accuracy: 19.48%\n",
            "Batch 400, Loss: 4.5005, Accuracy: 19.66%\n",
            "Batch 420, Loss: 4.9902, Accuracy: 19.74%\n",
            "Batch 440, Loss: 3.6042, Accuracy: 19.90%\n",
            "Batch 460, Loss: 1.8035, Accuracy: 19.93%\n",
            "Batch 480, Loss: 3.8157, Accuracy: 19.88%\n",
            "Batch 500, Loss: 4.1309, Accuracy: 20.09%\n",
            "Batch 520, Loss: 3.0650, Accuracy: 20.28%\n",
            "Batch 540, Loss: 2.6453, Accuracy: 20.52%\n",
            "Epoch 4: Train Loss: 3.5615, Train Acc: 20.47%\n",
            "Validation Accuracy: 0.00%\n",
            "Epoch 5\n",
            "Batch 20, Loss: 4.1905, Accuracy: 31.18%\n",
            "Batch 40, Loss: 6.2973, Accuracy: 26.97%\n",
            "Batch 60, Loss: 3.5064, Accuracy: 22.86%\n",
            "Batch 80, Loss: 3.1364, Accuracy: 22.56%\n",
            "Batch 100, Loss: 3.7137, Accuracy: 22.40%\n",
            "Batch 120, Loss: 3.8089, Accuracy: 20.96%\n",
            "Batch 140, Loss: 2.5787, Accuracy: 21.19%\n",
            "Batch 160, Loss: 3.6053, Accuracy: 20.69%\n",
            "Batch 180, Loss: 4.3846, Accuracy: 20.71%\n",
            "Batch 200, Loss: 3.2280, Accuracy: 20.99%\n",
            "Batch 220, Loss: 2.8677, Accuracy: 21.15%\n",
            "Batch 240, Loss: 4.1462, Accuracy: 21.17%\n",
            "Batch 260, Loss: 2.8047, Accuracy: 21.31%\n",
            "Batch 280, Loss: 3.0413, Accuracy: 21.40%\n",
            "Batch 300, Loss: 4.2213, Accuracy: 21.21%\n",
            "Batch 320, Loss: 3.1711, Accuracy: 20.75%\n",
            "Batch 340, Loss: 3.8707, Accuracy: 20.62%\n",
            "Batch 360, Loss: 3.4734, Accuracy: 20.61%\n",
            "Batch 380, Loss: 4.4813, Accuracy: 20.64%\n",
            "Batch 400, Loss: 2.6726, Accuracy: 20.79%\n",
            "Batch 420, Loss: 3.2454, Accuracy: 20.70%\n",
            "Batch 440, Loss: 3.1004, Accuracy: 20.89%\n",
            "Batch 460, Loss: 4.7905, Accuracy: 20.75%\n",
            "Batch 480, Loss: 3.0054, Accuracy: 21.07%\n",
            "Batch 500, Loss: 3.1008, Accuracy: 21.16%\n",
            "Batch 520, Loss: 3.5203, Accuracy: 20.86%\n",
            "Batch 540, Loss: 4.8267, Accuracy: 20.79%\n",
            "Epoch 5: Train Loss: 3.5007, Train Acc: 20.84%\n",
            "Validation Accuracy: 0.00%\n",
            "Best Validation Accuracy for human_action: 66.67%\n",
            "Model for human_action saved!\n",
            "Training model for brightness...\n",
            "Epoch 1\n",
            "Batch 20, Loss: 1.0945, Accuracy: 31.55%\n",
            "Batch 40, Loss: 0.8329, Accuracy: 42.12%\n",
            "Batch 60, Loss: 0.8894, Accuracy: 45.82%\n",
            "Batch 80, Loss: 1.1372, Accuracy: 48.81%\n",
            "Batch 100, Loss: 0.6755, Accuracy: 51.05%\n",
            "Batch 120, Loss: 0.8495, Accuracy: 52.60%\n",
            "Batch 140, Loss: 1.3014, Accuracy: 53.43%\n",
            "Batch 160, Loss: 0.6537, Accuracy: 54.40%\n",
            "Batch 180, Loss: 0.7370, Accuracy: 55.58%\n",
            "Batch 200, Loss: 1.0088, Accuracy: 56.19%\n",
            "Batch 220, Loss: 0.9595, Accuracy: 57.02%\n",
            "Batch 240, Loss: 0.8788, Accuracy: 57.81%\n",
            "Batch 260, Loss: 1.1975, Accuracy: 58.19%\n",
            "Batch 280, Loss: 0.8185, Accuracy: 58.34%\n",
            "Batch 300, Loss: 0.9015, Accuracy: 58.50%\n",
            "Batch 320, Loss: 1.6466, Accuracy: 58.91%\n",
            "Batch 340, Loss: 0.9432, Accuracy: 59.31%\n",
            "Batch 360, Loss: 1.1207, Accuracy: 59.53%\n",
            "Batch 380, Loss: 0.9957, Accuracy: 59.97%\n",
            "Batch 400, Loss: 0.9329, Accuracy: 60.03%\n",
            "Batch 420, Loss: 0.7139, Accuracy: 60.09%\n",
            "Batch 440, Loss: 0.7017, Accuracy: 60.03%\n",
            "Batch 460, Loss: 0.7462, Accuracy: 60.25%\n",
            "Batch 480, Loss: 0.7973, Accuracy: 60.48%\n",
            "Batch 500, Loss: 0.6500, Accuracy: 60.74%\n",
            "Batch 520, Loss: 0.9060, Accuracy: 60.56%\n",
            "Batch 540, Loss: 0.7549, Accuracy: 60.63%\n",
            "Epoch 1: Train Loss: 0.9060, Train Acc: 60.65%\n",
            "Validation Accuracy: 60.00%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 0.9688, Accuracy: 61.97%\n",
            "Batch 40, Loss: 0.6757, Accuracy: 62.24%\n",
            "Batch 60, Loss: 0.7536, Accuracy: 64.13%\n",
            "Batch 80, Loss: 0.6852, Accuracy: 63.55%\n",
            "Batch 100, Loss: 0.6280, Accuracy: 64.32%\n",
            "Batch 120, Loss: 0.6754, Accuracy: 63.80%\n",
            "Batch 140, Loss: 0.6751, Accuracy: 64.13%\n",
            "Batch 160, Loss: 0.7066, Accuracy: 64.25%\n",
            "Batch 180, Loss: 0.6871, Accuracy: 64.48%\n",
            "Batch 200, Loss: 1.2201, Accuracy: 64.59%\n",
            "Batch 220, Loss: 0.6568, Accuracy: 64.43%\n",
            "Batch 240, Loss: 0.6651, Accuracy: 64.76%\n",
            "Batch 260, Loss: 0.6504, Accuracy: 64.80%\n",
            "Batch 280, Loss: 0.4853, Accuracy: 65.12%\n",
            "Batch 300, Loss: 0.7449, Accuracy: 65.30%\n",
            "Batch 320, Loss: 1.0207, Accuracy: 65.38%\n",
            "Batch 340, Loss: 0.6368, Accuracy: 65.30%\n",
            "Batch 360, Loss: 0.6603, Accuracy: 65.33%\n",
            "Batch 380, Loss: 1.0521, Accuracy: 65.16%\n",
            "Batch 400, Loss: 0.9935, Accuracy: 65.22%\n",
            "Batch 420, Loss: 0.6857, Accuracy: 65.42%\n",
            "Batch 440, Loss: 1.2697, Accuracy: 65.44%\n",
            "Batch 460, Loss: 0.6896, Accuracy: 65.43%\n",
            "Batch 480, Loss: 0.7269, Accuracy: 65.46%\n",
            "Batch 500, Loss: 0.6623, Accuracy: 65.37%\n",
            "Batch 520, Loss: 0.7377, Accuracy: 65.44%\n",
            "Batch 540, Loss: 0.8436, Accuracy: 65.44%\n",
            "Epoch 2: Train Loss: 0.7879, Train Acc: 65.56%\n",
            "Validation Accuracy: 80.00%\n",
            "Epoch 3\n",
            "Batch 20, Loss: 0.5248, Accuracy: 69.17%\n",
            "Batch 40, Loss: 0.5563, Accuracy: 68.02%\n",
            "Batch 60, Loss: 0.5393, Accuracy: 69.62%\n",
            "Batch 80, Loss: 1.9091, Accuracy: 67.45%\n",
            "Batch 100, Loss: 0.5950, Accuracy: 68.67%\n",
            "Batch 120, Loss: 0.4628, Accuracy: 68.47%\n",
            "Batch 140, Loss: 0.5452, Accuracy: 69.09%\n",
            "Batch 160, Loss: 0.8695, Accuracy: 68.73%\n",
            "Batch 180, Loss: 0.5931, Accuracy: 68.97%\n",
            "Batch 200, Loss: 0.5782, Accuracy: 68.64%\n",
            "Batch 220, Loss: 0.4899, Accuracy: 68.81%\n",
            "Batch 240, Loss: 0.5788, Accuracy: 68.67%\n",
            "Batch 260, Loss: 0.4396, Accuracy: 68.72%\n",
            "Batch 280, Loss: 0.6850, Accuracy: 68.65%\n",
            "Batch 300, Loss: 0.5965, Accuracy: 68.77%\n",
            "Batch 320, Loss: 0.6531, Accuracy: 68.51%\n",
            "Batch 340, Loss: 1.6040, Accuracy: 68.13%\n",
            "Batch 360, Loss: 0.5792, Accuracy: 67.89%\n",
            "Batch 380, Loss: 0.5395, Accuracy: 68.01%\n",
            "Batch 400, Loss: 0.7771, Accuracy: 67.93%\n",
            "Batch 420, Loss: 0.7047, Accuracy: 67.76%\n",
            "Batch 440, Loss: 0.5331, Accuracy: 67.79%\n",
            "Batch 460, Loss: 0.8392, Accuracy: 67.76%\n",
            "Batch 480, Loss: 0.8137, Accuracy: 67.73%\n",
            "Batch 500, Loss: 1.0204, Accuracy: 67.66%\n",
            "Batch 520, Loss: 0.9714, Accuracy: 67.54%\n",
            "Batch 540, Loss: 0.7617, Accuracy: 67.42%\n",
            "Epoch 3: Train Loss: 0.7372, Train Acc: 67.30%\n",
            "Validation Accuracy: 65.00%\n",
            "Epoch 4\n",
            "Batch 20, Loss: 0.7671, Accuracy: 68.28%\n",
            "Batch 40, Loss: 1.0699, Accuracy: 65.34%\n",
            "Batch 60, Loss: 0.7148, Accuracy: 65.62%\n",
            "Batch 80, Loss: 0.5371, Accuracy: 67.23%\n",
            "Batch 100, Loss: 0.7299, Accuracy: 67.13%\n",
            "Batch 120, Loss: 0.6768, Accuracy: 68.05%\n",
            "Batch 140, Loss: 0.6172, Accuracy: 67.84%\n",
            "Batch 160, Loss: 0.6136, Accuracy: 67.91%\n",
            "Batch 180, Loss: 0.6716, Accuracy: 67.92%\n",
            "Batch 200, Loss: 0.5675, Accuracy: 67.74%\n",
            "Batch 220, Loss: 0.8654, Accuracy: 68.30%\n",
            "Batch 240, Loss: 0.7252, Accuracy: 68.49%\n",
            "Batch 260, Loss: 0.5131, Accuracy: 68.84%\n",
            "Batch 280, Loss: 0.6225, Accuracy: 68.98%\n",
            "Batch 300, Loss: 0.7459, Accuracy: 68.88%\n",
            "Batch 320, Loss: 0.7644, Accuracy: 68.22%\n",
            "Batch 340, Loss: 0.6352, Accuracy: 68.30%\n",
            "Batch 360, Loss: 0.4309, Accuracy: 68.31%\n",
            "Batch 380, Loss: 0.6601, Accuracy: 68.47%\n",
            "Batch 400, Loss: 0.6341, Accuracy: 68.43%\n",
            "Batch 420, Loss: 0.7169, Accuracy: 68.16%\n",
            "Batch 440, Loss: 0.8518, Accuracy: 68.16%\n",
            "Batch 460, Loss: 1.0989, Accuracy: 68.23%\n",
            "Batch 480, Loss: 0.7066, Accuracy: 68.27%\n",
            "Batch 500, Loss: 0.6071, Accuracy: 68.24%\n",
            "Batch 520, Loss: 0.4808, Accuracy: 68.08%\n",
            "Batch 540, Loss: 0.5702, Accuracy: 68.15%\n",
            "Epoch 4: Train Loss: 0.7203, Train Acc: 68.34%\n",
            "Validation Accuracy: 85.00%\n",
            "Epoch 5\n",
            "Batch 20, Loss: 0.9877, Accuracy: 69.64%\n",
            "Batch 40, Loss: 0.6575, Accuracy: 68.10%\n",
            "Batch 60, Loss: 0.7890, Accuracy: 69.55%\n",
            "Batch 80, Loss: 0.6079, Accuracy: 70.09%\n",
            "Batch 100, Loss: 0.7122, Accuracy: 70.37%\n",
            "Batch 120, Loss: 0.9340, Accuracy: 70.04%\n",
            "Batch 140, Loss: 0.6089, Accuracy: 69.30%\n",
            "Batch 160, Loss: 0.6139, Accuracy: 69.06%\n",
            "Batch 180, Loss: 0.6088, Accuracy: 69.62%\n",
            "Batch 200, Loss: 0.5223, Accuracy: 69.34%\n",
            "Batch 220, Loss: 0.6064, Accuracy: 69.27%\n",
            "Batch 240, Loss: 0.5809, Accuracy: 69.56%\n",
            "Batch 260, Loss: 0.6736, Accuracy: 69.35%\n",
            "Batch 280, Loss: 0.8800, Accuracy: 69.35%\n",
            "Batch 300, Loss: 0.4919, Accuracy: 69.51%\n",
            "Batch 320, Loss: 0.5181, Accuracy: 69.62%\n",
            "Batch 340, Loss: 0.4989, Accuracy: 69.54%\n",
            "Batch 360, Loss: 0.4661, Accuracy: 69.61%\n",
            "Batch 380, Loss: 0.5897, Accuracy: 69.62%\n",
            "Batch 400, Loss: 0.6789, Accuracy: 69.55%\n",
            "Batch 420, Loss: 0.4981, Accuracy: 69.64%\n",
            "Batch 440, Loss: 0.6285, Accuracy: 69.51%\n",
            "Batch 460, Loss: 0.7569, Accuracy: 69.24%\n",
            "Batch 480, Loss: 1.6371, Accuracy: 69.20%\n",
            "Batch 500, Loss: 0.5438, Accuracy: 69.18%\n",
            "Batch 520, Loss: 0.5219, Accuracy: 69.28%\n",
            "Batch 540, Loss: 0.5214, Accuracy: 69.32%\n",
            "Epoch 5: Train Loss: 0.7006, Train Acc: 69.49%\n",
            "Validation Accuracy: 80.00%\n",
            "Best Validation Accuracy for brightness: 85.00%\n",
            "Model for brightness saved!\n",
            "Training model for colorfulness...\n",
            "Epoch 1\n",
            "Batch 20, Loss: 1.6982, Accuracy: 26.81%\n",
            "Batch 40, Loss: 1.6297, Accuracy: 31.75%\n",
            "Batch 60, Loss: 1.4561, Accuracy: 34.81%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attr, num_classes \u001b[38;5;129;01min\u001b[39;00m attribute_classes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m     trained_models[attr] \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_attribute_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mtrain_attribute_model\u001b[1;34m(train_dataset, valid_dataset, num_classes, attribute_name, epochs)\u001b[0m\n\u001b[0;32m     41\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     42\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     46\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "attribute_classes = {\n",
        "    'human_action': 264,\n",
        "    'brightness': 11,\n",
        "    'colorfulness': 11,\n",
        "    'object': 409\n",
        "}\n",
        "trained_models = {}\n",
        "for attr, num_classes in attribute_classes.items():\n",
        "    print(f\"Training model for {attr}...\")\n",
        "    trained_models[attr] = train_attribute_model(train_dataset, valid_dataset, num_classes, attr,  epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training model for colorfulness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "Batch 20, Loss: 1.6503, Accuracy: 26.70%\n",
            "Batch 40, Loss: 1.4592, Accuracy: 30.83%\n",
            "Batch 60, Loss: 1.4419, Accuracy: 33.16%\n",
            "Batch 80, Loss: 1.5101, Accuracy: 35.34%\n",
            "Batch 100, Loss: 1.0453, Accuracy: 37.12%\n",
            "Batch 120, Loss: 1.1581, Accuracy: 37.94%\n",
            "Batch 140, Loss: 0.9886, Accuracy: 38.93%\n",
            "Batch 160, Loss: 1.3310, Accuracy: 40.01%\n",
            "Batch 180, Loss: 1.1190, Accuracy: 40.13%\n",
            "Batch 200, Loss: 1.0375, Accuracy: 40.73%\n",
            "Batch 220, Loss: 1.3488, Accuracy: 41.14%\n",
            "Batch 240, Loss: 1.1728, Accuracy: 41.93%\n",
            "Batch 260, Loss: 1.4217, Accuracy: 42.21%\n",
            "Batch 280, Loss: 0.9751, Accuracy: 42.51%\n",
            "Batch 300, Loss: 1.1574, Accuracy: 42.85%\n",
            "Batch 320, Loss: 1.6871, Accuracy: 42.93%\n",
            "Batch 340, Loss: 1.2241, Accuracy: 43.10%\n",
            "Batch 360, Loss: 1.0774, Accuracy: 43.09%\n",
            "Batch 380, Loss: 1.2113, Accuracy: 43.21%\n",
            "Batch 400, Loss: 1.2370, Accuracy: 43.59%\n",
            "Batch 420, Loss: 0.9570, Accuracy: 44.11%\n",
            "Batch 440, Loss: 1.3851, Accuracy: 44.36%\n",
            "Batch 460, Loss: 1.5444, Accuracy: 44.40%\n",
            "Batch 480, Loss: 1.4793, Accuracy: 44.68%\n",
            "Batch 500, Loss: 1.1472, Accuracy: 44.85%\n",
            "Batch 520, Loss: 0.9115, Accuracy: 45.19%\n",
            "Batch 540, Loss: 1.1069, Accuracy: 45.35%\n",
            "Epoch 1: Train Loss: 1.2838, Train Acc: 45.55%\n",
            "Validation Accuracy: 35.00%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 1.1229, Accuracy: 53.63%\n",
            "Batch 40, Loss: 1.1245, Accuracy: 51.98%\n",
            "Batch 60, Loss: 1.2306, Accuracy: 51.86%\n",
            "Batch 80, Loss: 1.1852, Accuracy: 50.97%\n",
            "Batch 100, Loss: 1.1536, Accuracy: 49.89%\n",
            "Batch 120, Loss: 1.1129, Accuracy: 49.92%\n",
            "Batch 140, Loss: 1.1761, Accuracy: 49.52%\n",
            "Batch 160, Loss: 1.2383, Accuracy: 49.63%\n",
            "Batch 180, Loss: 1.1899, Accuracy: 50.02%\n",
            "Batch 200, Loss: 0.9614, Accuracy: 49.97%\n",
            "Batch 220, Loss: 1.1046, Accuracy: 50.14%\n",
            "Batch 240, Loss: 1.3623, Accuracy: 50.09%\n",
            "Batch 260, Loss: 0.9237, Accuracy: 50.21%\n",
            "Batch 280, Loss: 1.2694, Accuracy: 50.29%\n",
            "Batch 300, Loss: 1.0665, Accuracy: 50.48%\n",
            "Batch 320, Loss: 0.9788, Accuracy: 50.36%\n",
            "Batch 340, Loss: 1.1640, Accuracy: 50.34%\n",
            "Batch 360, Loss: 1.4004, Accuracy: 50.45%\n",
            "Batch 380, Loss: 1.1352, Accuracy: 50.53%\n",
            "Batch 400, Loss: 2.0569, Accuracy: 50.60%\n",
            "Batch 420, Loss: 0.7426, Accuracy: 50.80%\n",
            "Batch 440, Loss: 0.9852, Accuracy: 50.96%\n",
            "Batch 460, Loss: 1.3188, Accuracy: 50.82%\n",
            "Batch 480, Loss: 1.4019, Accuracy: 50.99%\n",
            "Batch 500, Loss: 1.0428, Accuracy: 51.08%\n",
            "Batch 520, Loss: 1.1934, Accuracy: 50.99%\n",
            "Batch 540, Loss: 1.0584, Accuracy: 51.11%\n",
            "Epoch 2: Train Loss: 1.1034, Train Acc: 51.15%\n",
            "Validation Accuracy: 40.00%\n",
            "Best Validation Accuracy for colorfulness: 40.00%\n",
            "Model for colorfulness saved!\n",
            "Training model for facial_expression...\n",
            "Epoch 1\n",
            "Batch 20, Loss: 0.7729, Accuracy: 68.82%\n",
            "Batch 40, Loss: 2.1405, Accuracy: 66.50%\n",
            "Batch 60, Loss: 0.6940, Accuracy: 70.99%\n",
            "Batch 80, Loss: 1.0736, Accuracy: 71.90%\n",
            "Batch 100, Loss: 0.4291, Accuracy: 72.34%\n",
            "Batch 120, Loss: 0.4351, Accuracy: 70.69%\n",
            "Batch 140, Loss: 0.6791, Accuracy: 71.89%\n",
            "Batch 160, Loss: 0.5680, Accuracy: 72.51%\n",
            "Batch 180, Loss: 2.1864, Accuracy: 71.78%\n",
            "Batch 200, Loss: 0.8394, Accuracy: 72.34%\n",
            "Batch 220, Loss: 1.3768, Accuracy: 71.73%\n",
            "Batch 240, Loss: 1.6104, Accuracy: 71.45%\n",
            "Batch 260, Loss: 0.4294, Accuracy: 72.02%\n",
            "Batch 280, Loss: 1.5013, Accuracy: 72.41%\n",
            "Batch 300, Loss: 0.2794, Accuracy: 71.99%\n",
            "Batch 320, Loss: 1.9273, Accuracy: 72.06%\n",
            "Batch 340, Loss: 1.1785, Accuracy: 72.39%\n",
            "Batch 360, Loss: 0.6481, Accuracy: 72.31%\n",
            "Batch 380, Loss: 2.6795, Accuracy: 72.28%\n",
            "Batch 400, Loss: 1.0759, Accuracy: 72.62%\n",
            "Batch 420, Loss: 0.2577, Accuracy: 72.70%\n",
            "Batch 440, Loss: 0.6054, Accuracy: 72.67%\n",
            "Batch 460, Loss: 0.6744, Accuracy: 72.86%\n",
            "Batch 480, Loss: 1.1590, Accuracy: 73.04%\n",
            "Batch 500, Loss: 1.2483, Accuracy: 72.41%\n",
            "Batch 520, Loss: 1.9828, Accuracy: 72.61%\n",
            "Batch 540, Loss: 1.1700, Accuracy: 72.43%\n",
            "Epoch 1: Train Loss: 1.0087, Train Acc: 72.33%\n",
            "Validation Accuracy: 66.67%\n",
            "Epoch 2\n",
            "Batch 20, Loss: 0.7537, Accuracy: 69.90%\n",
            "Batch 40, Loss: 0.3546, Accuracy: 72.55%\n",
            "Batch 60, Loss: 0.3358, Accuracy: 73.36%\n",
            "Batch 80, Loss: 0.3544, Accuracy: 74.12%\n",
            "Batch 100, Loss: 0.6554, Accuracy: 72.73%\n",
            "Batch 120, Loss: 1.3532, Accuracy: 73.58%\n",
            "Batch 140, Loss: 0.1205, Accuracy: 74.04%\n",
            "Batch 160, Loss: 0.2687, Accuracy: 74.14%\n",
            "Batch 180, Loss: 0.2798, Accuracy: 74.62%\n",
            "Batch 200, Loss: 1.8024, Accuracy: 75.08%\n",
            "Batch 220, Loss: 1.5165, Accuracy: 74.83%\n",
            "Batch 240, Loss: 0.8830, Accuracy: 75.18%\n",
            "Batch 260, Loss: 4.0979, Accuracy: 74.90%\n",
            "Batch 280, Loss: 0.4884, Accuracy: 74.43%\n",
            "Batch 300, Loss: 0.1448, Accuracy: 74.79%\n",
            "Batch 320, Loss: 1.0035, Accuracy: 74.97%\n",
            "Batch 340, Loss: 3.6472, Accuracy: 74.95%\n",
            "Batch 360, Loss: 0.1394, Accuracy: 75.22%\n",
            "Batch 380, Loss: 0.9878, Accuracy: 75.08%\n",
            "Batch 400, Loss: 0.7995, Accuracy: 74.86%\n",
            "Batch 420, Loss: 0.3370, Accuracy: 74.92%\n",
            "Batch 440, Loss: 0.9013, Accuracy: 74.51%\n",
            "Batch 460, Loss: 0.2905, Accuracy: 74.62%\n",
            "Batch 480, Loss: 0.6638, Accuracy: 74.26%\n",
            "Batch 500, Loss: 0.8589, Accuracy: 74.47%\n",
            "Batch 520, Loss: 1.1550, Accuracy: 74.42%\n",
            "Batch 540, Loss: 1.8986, Accuracy: 74.08%\n",
            "Epoch 2: Train Loss: 0.9312, Train Acc: 74.10%\n",
            "Validation Accuracy: 100.00%\n",
            "Best Validation Accuracy for facial_expression: 100.00%\n",
            "Model for facial_expression saved!\n"
          ]
        }
      ],
      "source": [
        "attribute_classes = {\n",
        "    'colorfulness': 11,\n",
        "    'facial_expression': 6\n",
        "}\n",
        "trained_models = {}\n",
        "for attr, num_classes in attribute_classes.items():\n",
        "    print(f\"Training model for {attr}...\")\n",
        "    trained_models[attr] = train_attribute_model(train_dataset, valid_dataset, num_classes, attr,  epochs=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1=trained_models['colorfulness']\n",
        "torch.save(model1.state_dict(), f\"colorfulness_predictor.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "model1=trained_models['facial_expression']\n",
        "torch.save(model1.state_dict(), f\"facial_expression_predictor.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "attribute_classes = {\n",
        "    'scene': 254,\n",
        "    'facial_expression': 6,\n",
        "    'human_action': 264,\n",
        "    'brightness': 11,\n",
        "    'colorfulness': 11,\n",
        "    'object': 409\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
