{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "class EmoSet(Dataset):\n",
        "    ATTRIBUTES_MULTI_CLASS = [\n",
        "        'scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness',\n",
        "    ]\n",
        "    ATTRIBUTES_MULTI_LABEL = [\n",
        "        'object'\n",
        "    ]\n",
        "    NUM_CLASSES = {\n",
        "        'brightness': 11,\n",
        "        'colorfulness': 11,\n",
        "        'scene': 254,\n",
        "        'object': 409,\n",
        "        'facial_expression': 6,\n",
        "        'human_action': 264,\n",
        "    }\n",
        "    def __init__(self,\n",
        "                 data_root,\n",
        "                 num_emotion_classes,\n",
        "                 phase,\n",
        "                 ):\n",
        "        assert num_emotion_classes in (8, 2)\n",
        "        assert phase in ('train', 'val', 'test')\n",
        "        self.transforms_dict = self.get_data_transforms()\n",
        "        self.info = self.get_info(data_root, num_emotion_classes)\n",
        "        if phase == 'train':\n",
        "            self.transform = self.transforms_dict['train']\n",
        "        elif phase == 'val':\n",
        "            self.transform = self.transforms_dict['val']\n",
        "        elif phase == 'test':\n",
        "            self.transform = self.transforms_dict['test']\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        data_store = json.load(open(os.path.join(data_root, f'{phase}.json')))\n",
        "        self.data_store = [\n",
        "            [\n",
        "                self.info['emotion']['label2idx'][item[0]],\n",
        "                os.path.join(data_root, item[1]),\n",
        "                os.path.join(data_root, item[2])\n",
        "            ]\n",
        "            for item in data_store\n",
        "        ]\n",
        "    @classmethod\n",
        "    def get_data_transforms(cls):\n",
        "        transforms_dict = {\n",
        "            'train': transforms.Compose([\n",
        "                transforms.RandomResizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'val': transforms.Compose([\n",
        "                transforms.Resize(224),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "            'test': transforms.Compose([\n",
        "                transforms.Resize(224),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "            ]),\n",
        "        }\n",
        "        return transforms_dict\n",
        "    def get_info(self, data_root, num_emotion_classes):\n",
        "        assert num_emotion_classes in (8, 2)\n",
        "        info = json.load(open(os.path.join(data_root, 'info.json')))\n",
        "        if num_emotion_classes == 8:\n",
        "            pass\n",
        "        elif num_emotion_classes == 2:\n",
        "            emotion_info = {\n",
        "                'label2idx': {\n",
        "                    'amusement': 0,\n",
        "                    'awe': 0,\n",
        "                    'contentment': 0,\n",
        "                    'excitement': 0,\n",
        "                    'anger': 1,\n",
        "                    'disgust': 1,\n",
        "                    'fear': 1,\n",
        "                    'sadness': 1,\n",
        "                },\n",
        "                'idx2label': {\n",
        "                    '0': 'positive',\n",
        "                    '1': 'negative',\n",
        "                }\n",
        "            }\n",
        "            info['emotion'] = emotion_info\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        return info\n",
        "    def load_image_by_path(self, path):\n",
        "        image = Image.open(path).convert('RGB')\n",
        "        image = self.transform(image)\n",
        "        return image\n",
        "    def load_annotation_by_path(self, path):\n",
        "        json_data = json.load(open(path))\n",
        "        return json_data\n",
        "    def __getitem__(self, item):\n",
        "        emotion_label_idx, image_path, annotation_path = self.data_store[item]\n",
        "        image = self.load_image_by_path(image_path)\n",
        "        annotation_data = self.load_annotation_by_path(annotation_path)\n",
        "        data = { 'image': image, 'emotion_label_idx': emotion_label_idx}\n",
        "        for attribute in self.ATTRIBUTES_MULTI_CLASS:\n",
        "            attribute_label_idx = -1\n",
        "            if attribute in annotation_data:\n",
        "                attribute_label_idx = self.info[attribute]['label2idx'][str(annotation_data[attribute])]\n",
        "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
        "        for attribute in self.ATTRIBUTES_MULTI_LABEL:\n",
        "            assert attribute == 'object'\n",
        "            num_classes = self.NUM_CLASSES[attribute]\n",
        "            attribute_label_idx = torch.zeros(num_classes)\n",
        "            if attribute in annotation_data:\n",
        "                for label in annotation_data[attribute]:\n",
        "                    attribute_label_idx[self.info[attribute]['label2idx'][label]] = 1\n",
        "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
        "        return data\n",
        "    def __len__(self):\n",
        "        return len(self.data_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\HP\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "class EmotionRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_emotions=8, attribute_sizes=None, embedding_dim=32, hidden_dim=128):\n",
        "        super(EmotionRecognitionModel, self).__init__()\n",
        "        self.swin = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "        swin_feature_dim = self.swin.num_features\n",
        "        self.image_fc = nn.Linear(swin_feature_dim, hidden_dim)\n",
        "        self.attribute_nets = nn.ModuleDict()\n",
        "        self.attribute_sizes = attribute_sizes\n",
        "        for attr, size in attribute_sizes.items():\n",
        "            if attr != 'object':  \n",
        "                self.attribute_nets[attr] = nn.Sequential(\n",
        "                    nn.Embedding(size + 1, embedding_dim, padding_idx=size),\n",
        "                    nn.Linear(embedding_dim, hidden_dim),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "        self.object_fc = nn.Linear(attribute_sizes['object'], hidden_dim)\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * (len(attribute_sizes) + 1), hidden_dim),  \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim, num_emotions)\n",
        "    def forward(self, image, attributes):\n",
        "        image_features = self.swin(image)\n",
        "        image_features = self.image_fc(image_features)\n",
        "        attr_features = []\n",
        "        for attr, net in self.attribute_nets.items():\n",
        "            attr_values = attributes[attr].clone()\n",
        "            attr_values[attr_values == -1] = self.attribute_sizes[attr]\n",
        "            attr_features.append(net(attr_values))  \n",
        "        object_features = self.object_fc(attributes['object'])\n",
        "        attr_features.append(object_features)\n",
        "        combined_features = torch.cat([image_features] + attr_features, dim=1)\n",
        "        fused_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating Swin Transformer (with Attributes) on test set...\n",
            "\n",
            "Evaluation Results:\n",
            "Loss: 1.2544\n",
            "Accuracy: 53.25%\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1509  102  551  285   20  168  123   12]\n",
            " [ 228 1118  448   75   34  237   81   39]\n",
            " [ 412  175 1259  166   62  297   81   32]\n",
            " [ 372   92  333 2021   33   91   62   10]\n",
            " [ 165   45  200   54  773  261  155   55]\n",
            " [ 196   23  168    5   37  960  216   56]\n",
            " [ 220   36  136   81   89  452  839  121]\n",
            " [  74   62  204   34   73  169  274  955]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   amusement       0.48      0.54      0.51      2770\n",
            "         awe       0.68      0.49      0.57      2260\n",
            " contentment       0.38      0.51      0.44      2484\n",
            "  excitement       0.74      0.67      0.70      3014\n",
            "       anger       0.69      0.45      0.55      1708\n",
            "     disgust       0.36      0.58      0.45      1661\n",
            "        fear       0.46      0.43      0.44      1974\n",
            "     sadness       0.75      0.52      0.61      1845\n",
            "\n",
            "    accuracy                           0.53     17716\n",
            "   macro avg       0.57      0.52      0.53     17716\n",
            "weighted avg       0.57      0.53      0.54     17716\n",
            "\n",
            "Test Accuracy: 53.25%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "def process_batch(model, batch, device):\n",
        "    \"\"\"Helper function to process a batch for evaluation (with attributes)\"\"\"\n",
        "    images = batch['image'].to(device)\n",
        "    emotion_labels = batch['emotion_label_idx'].to(device)\n",
        "    attributes = {}\n",
        "    for attr in ['scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness']:\n",
        "        attributes[attr] = batch[f'{attr}_label_idx'].to(device)\n",
        "    attributes['object'] = batch['object_label_idx'].to(device)\n",
        "    outputs = model(images, attributes)\n",
        "    return outputs, emotion_labels, attributes\n",
        "def evaluate_model(model, dataloader, criterion, device='cuda', idx2label=None):\n",
        "    \"\"\"Evaluate the model and print detailed metrics\"\"\"\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_labels = []\n",
        "    all_predictions = []\n",
        "    with torch.no_grad():\n",
        "        for i, batch in enumerate(dataloader):\n",
        "            try:\n",
        "                outputs, emotion_labels, _ = process_batch(model, batch, device)\n",
        "                loss = criterion(outputs, emotion_labels)\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                all_labels.extend(emotion_labels.cpu().numpy())\n",
        "                all_predictions.extend(predicted.cpu().numpy())\n",
        "            except Exception as e:\n",
        "                print(f\"Error in evaluation batch {i}: {e}\")\n",
        "                continue\n",
        "    test_loss = running_loss / len(dataloader)\n",
        "    test_accuracy = 100 * sum(1 for pred, label in zip(all_predictions, all_labels) if pred == label) / len(all_labels)\n",
        "    print(f\"\\nEvaluation Results:\")\n",
        "    print(f\"Loss: {test_loss:.4f}\")\n",
        "    print(f\"Accuracy: {test_accuracy:.2f}%\")\n",
        "    cm = confusion_matrix(all_labels, all_predictions)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    if idx2label:\n",
        "        target_names = [idx2label[str(i)] for i in range(len(idx2label))]\n",
        "        print(classification_report(all_labels, all_predictions, target_names=target_names, zero_division=0))\n",
        "    else:\n",
        "        print(classification_report(all_labels, all_predictions, zero_division=0))\n",
        "    return test_loss, test_accuracy, cm, all_labels, all_predictions\n",
        "if __name__ == \"__main__\":\n",
        "    data_root = \"C:/Users/HP/Downloads/Emotion-Analysis/dataset\"\n",
        "    num_emotion_classes = 8\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    attribute_sizes = {\n",
        "        'scene': 254,\n",
        "        'facial_expression': 6,\n",
        "        'human_action': 264,\n",
        "        'brightness': 11,\n",
        "        'colorfulness': 11,\n",
        "        'object': 409\n",
        "    }\n",
        "    train_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='train')\n",
        "    test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    model = EmotionRecognitionModel(num_emotions=num_emotion_classes, attribute_sizes=attribute_sizes).to(device)\n",
        "    checkpoint_path = \"C:/Users/HP/Downloads/Emotion-Analysis/emotion_recognition_epoch_8.pth\"\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print(\"\\nEvaluating Swin Transformer (with Attributes) on test set...\")\n",
        "    emotion_idx2label = train_dataset.info['emotion']['idx2label']\n",
        "    test_loss, test_acc, conf_matrix, _, _ = evaluate_model(\n",
        "        model, \n",
        "        test_dataloader, \n",
        "        criterion, \n",
        "        device=device, \n",
        "        idx2label=emotion_idx2label\n",
        "    )\n",
        "    print(f\"Test Accuracy: {test_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import os\n",
        "class EmotionRecognitionModel(nn.Module):\n",
        "    def __init__(self, num_emotions=8, hidden_dim=128):\n",
        "        super(EmotionRecognitionModel, self).__init__()\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n",
        "        self.image_fc = nn.Linear(resnet.fc.in_features, hidden_dim)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.classifier = nn.Linear(hidden_dim, num_emotions)\n",
        "    def forward(self, image):\n",
        "        image_features = self.cnn(image)\n",
        "        image_features = image_features.view(image_features.size(0), -1)\n",
        "        image_features = self.image_fc(image_features)\n",
        "        image_features = self.dropout(image_features)\n",
        "        output = self.classifier(image_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image-Only Emotion Recognition Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   amusement       0.44      0.50      0.47      2770\n",
            "       anger       0.60      0.63      0.61      2260\n",
            "         awe       0.38      0.37      0.38      2484\n",
            " contentment       0.48      0.61      0.54      3014\n",
            "     disgust       0.44      0.30      0.36      1708\n",
            "  excitement       0.56      0.40      0.46      1661\n",
            "        fear       0.43      0.44      0.43      1974\n",
            "     sadness       0.41      0.36      0.38      1845\n",
            "\n",
            "    accuracy                           0.47     17716\n",
            "   macro avg       0.47      0.45      0.45     17716\n",
            "weighted avg       0.47      0.47      0.46     17716\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "def process_batch(model, batch, device):\n",
        "    \"\"\"Helper function to process a batch for either training or validation\"\"\"\n",
        "    images = batch['image'].to(device)\n",
        "    emotion_labels = batch['emotion_label_idx'].to(device)\n",
        "    outputs = model(images)\n",
        "    return outputs, emotion_labels\n",
        "test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "model = EmotionRecognitionModel(num_emotions=num_emotion_classes, hidden_dim=128).to(device)\n",
        "checkpoint_path = \"C:/Users/HP/Downloads/Emotion-Analysis/emotion_recognition_image_only_epoch_1.pth\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        outputs, emotion_labels = process_batch(model, batch, device)\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(emotion_labels.cpu().numpy())\n",
        "emotion_labels = ['amusement', 'anger', 'awe', 'contentment', 'disgust', 'excitement', 'fear', 'sadness']\n",
        "print(\"Image-Only Emotion Recognition Classification Report:\")\n",
        "print(classification_report(all_labels, all_preds, target_names=emotion_labels, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
