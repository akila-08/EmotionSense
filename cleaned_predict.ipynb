{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_emotion_with_attributes(image_tensor, attribute_models, emotion_model):\n",
        "    \n",
        "    image_tensor = image_tensor\n",
        "    emotion_model.eval()\n",
        "    for model in attribute_models.values():\n",
        "        model.eval()\n",
        "    attributes = {}\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for attr_name, attr_model in attribute_models.items():\n",
        "            if attr_name != 'object':\n",
        "                attr_model = attr_model\n",
        "                outputs = attr_model(image_tensor)\n",
        "                pred = outputs.argmax(dim=1)\n",
        "                attributes[attr_name] = pred\n",
        "        \n",
        "        num_objects = 409\n",
        "        random_objects = torch.zeros(1, num_objects)\n",
        "        random_indices = torch.randint(0, num_objects, (5,))\n",
        "        random_objects[0, random_indices] = 1.0\n",
        "        attributes['object'] = random_objects\n",
        "        emotion_output = emotion_model(image_tensor, attributes)\n",
        "        emotion_pred = emotion_output.argmax(dim=1)\n",
        "    \n",
        "    return emotion_pred.item(), attributes\n",
        "\n",
        "\n",
        "def process_single_image(image_path, attribute_models, emotion_model, idx2emotion):\n",
        "    \n",
        "    from PIL import Image\n",
        "    from torchvision import transforms\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(224),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    \n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    image_tensor = transform(image).unsqueeze(0)\n",
        "    emotion_idx, attributes = predict_emotion_with_attributes(\n",
        "        image_tensor, attribute_models, emotion_model\n",
        "    )\n",
        "    emotion_label = idx2emotion[str(emotion_idx)]\n",
        "    print(f\"Predicted emotion: {emotion_label}\")\n",
        "    return emotion_label, attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import timm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class EmotionRecognitionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_emotions=8, attribute_sizes=None, embedding_dim=32, hidden_dim=128):\n",
        "        super(EmotionRecognitionModel, self).__init__()\n",
        "        self.swin = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=0)\n",
        "        swin_feature_dim = self.swin.num_features  \n",
        "        self.image_fc = nn.Linear(swin_feature_dim, hidden_dim)\n",
        "        self.attribute_nets = nn.ModuleDict()\n",
        "        self.attribute_sizes = attribute_sizes\n",
        "        for attr, size in attribute_sizes.items():\n",
        "            if attr != 'object':  \n",
        "                self.attribute_nets[attr] = nn.Sequential(\n",
        "                    nn.Embedding(size + 1, embedding_dim, padding_idx=size), \n",
        "                    nn.Linear(embedding_dim, hidden_dim),\n",
        "                    nn.ReLU()\n",
        "                )\n",
        "        self.object_fc = nn.Linear(attribute_sizes['object'], hidden_dim)\n",
        "        self.fusion_fc = nn.Sequential(\n",
        "            nn.Linear(hidden_dim * (len(attribute_sizes) + 1), hidden_dim),  \n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_dim, num_emotions)\n",
        "    \n",
        "    def forward(self, image, attributes):\n",
        "        image_features = self.swin(image)\n",
        "        image_features = self.image_fc(image_features)\n",
        "        attr_features = []\n",
        "        for attr, net in self.attribute_nets.items():\n",
        "            attr_values = attributes[attr].clone()\n",
        "            attr_values[attr_values == -1] = self.attribute_sizes[attr]  \n",
        "            attr_features.append(net(attr_values))  \n",
        "        object_features = self.object_fc(attributes['object'])\n",
        "        attr_features.append(object_features)\n",
        "        combined_features = torch.cat([image_features] + attr_features, dim=1)\n",
        "        fused_features = self.fusion_fc(combined_features)\n",
        "        output = self.classifier(fused_features)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class AttributePredictor(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(AttributePredictor, self).__init__()\n",
        "        resnet = models.resnet18(pretrained=True) \n",
        "        self.feature_extractor = nn.Sequential(*list(resnet.children())[:-1])  \n",
        "        self.fc = nn.Linear(resnet.fc.in_features, num_classes)  \n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = x.view(x.size(0), -1) \n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted emotion: excitement\n",
            "Final emotion prediction: excitement\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import json\n",
        "\n",
        "def load_emotion_model(model_path):\n",
        "    checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "    attribute_sizes = checkpoint['attribute_sizes']\n",
        "    num_emotions = checkpoint['num_emotions']\n",
        "    model = EmotionRecognitionModel(\n",
        "        num_emotions=num_emotions, \n",
        "        attribute_sizes=attribute_sizes\n",
        "    )\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model = model\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "def main():\n",
        "    emotion_model = load_emotion_model('D:/sem 6/ai/package/image/swin transformer/saved_models/emotion_recognition_epoch_8.pth')\n",
        "    attribute_classes = {\n",
        "    'scene': 254,\n",
        "    'facial_expression': 6,\n",
        "    'human_action': 264,\n",
        "    'brightness': 11,\n",
        "    'colorfulness': 11,\n",
        "    'object': 409  \n",
        "    }\n",
        "    attribute_models = {}\n",
        "    for attr in ['scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness']:\n",
        "        model = AttributePredictor(attribute_classes[attr])\n",
        "        model.load_state_dict(torch.load(f\"D:/sem 6/ai/package/image/attributes/{attr}_predictor.pth\"))\n",
        "        attribute_models[attr] = model\n",
        "    \n",
        "    with open('D:/sem 6/ai/package/image/dataset/info.json', 'r') as f:\n",
        "        info = json.load(f)\n",
        "    idx2emotion = info['emotion']['idx2label']\n",
        "    image_path = 'D:/sem 6/ai/package/image/dataset/image/excitement/excitement_00013.jpg' \n",
        "    emotion, _ = process_single_image(image_path, attribute_models, emotion_model, idx2emotion)\n",
        "    print(f\"Final emotion prediction: {emotion}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
