{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import json\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "class EmoSet(Dataset):\n",
    "    ATTRIBUTES_MULTI_CLASS = [\n",
    "        'scene', 'facial_expression', 'human_action', 'brightness', 'colorfulness',\n",
    "    ]\n",
    "    ATTRIBUTES_MULTI_LABEL = [\n",
    "        'object'\n",
    "    ]\n",
    "    NUM_CLASSES = {\n",
    "        'brightness': 11,\n",
    "        'colorfulness': 11,\n",
    "        'scene': 254,\n",
    "        'object': 409,\n",
    "        'facial_expression': 6,\n",
    "        'human_action': 264,\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 data_root,\n",
    "                 num_emotion_classes,\n",
    "                 phase,\n",
    "                 ):\n",
    "        assert num_emotion_classes in (8, 2)\n",
    "        assert phase in ('train', 'val', 'test')\n",
    "        self.transforms_dict = self.get_data_transforms()\n",
    "\n",
    "        self.info = self.get_info(data_root, num_emotion_classes)\n",
    "\n",
    "        if phase == 'train':\n",
    "            self.transform = self.transforms_dict['train']\n",
    "        elif phase == 'val':\n",
    "            self.transform = self.transforms_dict['val']\n",
    "        elif phase == 'test':\n",
    "            self.transform = self.transforms_dict['test']\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        data_store = json.load(open(os.path.join(data_root, f'{phase}.json')))\n",
    "        self.data_store = [\n",
    "            [\n",
    "                self.info['emotion']['label2idx'][item[0]],\n",
    "    \n",
    "                os.path.join(data_root, item[1]),\n",
    "                os.path.join(data_root, item[2])\n",
    "            ]\n",
    "            for item in data_store\n",
    "        ]\n",
    "\n",
    "    @classmethod\n",
    "    def get_data_transforms(cls):\n",
    "        transforms_dict = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(224),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(224),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "        return transforms_dict\n",
    "\n",
    "    def get_info(self, data_root, num_emotion_classes):\n",
    "        assert num_emotion_classes in (8, 2)\n",
    "        info = json.load(open(os.path.join(data_root, 'info.json')))\n",
    "        if num_emotion_classes == 8:\n",
    "            pass\n",
    "        elif num_emotion_classes == 2:\n",
    "            emotion_info = {\n",
    "                'label2idx': {\n",
    "                    'amusement': 0,\n",
    "                    'awe': 0,\n",
    "                    'contentment': 0,\n",
    "                    'excitement': 0,\n",
    "                    'anger': 1,\n",
    "                    'disgust': 1,\n",
    "                    'fear': 1,\n",
    "                    'sadness': 1,\n",
    "                },\n",
    "                'idx2label': {\n",
    "                    '0': 'positive',\n",
    "                    '1': 'negative',\n",
    "                }\n",
    "            }\n",
    "            info['emotion'] = emotion_info\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        return info\n",
    "\n",
    "    def load_image_by_path(self, path):\n",
    "        image = Image.open(path).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def load_annotation_by_path(self, path):\n",
    "        json_data = json.load(open(path))\n",
    "        return json_data\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        emotion_label_idx, image_path, annotation_path = self.data_store[item]\n",
    "        image = self.load_image_by_path(image_path)\n",
    "        annotation_data = self.load_annotation_by_path(annotation_path)\n",
    "        data = { 'image': image, 'emotion_label_idx': emotion_label_idx}\n",
    "\n",
    "        for attribute in self.ATTRIBUTES_MULTI_CLASS:\n",
    "            # if empty, set to -1, else set to label index\n",
    "            attribute_label_idx = -1\n",
    "            if attribute in annotation_data:\n",
    "                attribute_label_idx = self.info[attribute]['label2idx'][str(annotation_data[attribute])]\n",
    "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
    "\n",
    "        for attribute in self.ATTRIBUTES_MULTI_LABEL:\n",
    "            # if empty, set to 0, else set to 1\n",
    "            assert attribute == 'object'\n",
    "            num_classes = self.NUM_CLASSES[attribute]\n",
    "            attribute_label_idx = torch.zeros(num_classes)\n",
    "            if attribute in annotation_data:\n",
    "                for label in annotation_data[attribute]:\n",
    "                    attribute_label_idx[self.info[attribute]['label2idx'][label]] = 1\n",
    "            data.update({f'{attribute}_label_idx': attribute_label_idx})\n",
    "\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_store)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_root = \"dataset\"\n",
    "    num_emotion_classes = 8\n",
    "    phase = 'train'\n",
    "\n",
    "    dataset = EmoSet(\n",
    "        data_root=data_root,\n",
    "        num_emotion_classes=num_emotion_classes,\n",
    "        phase=phase,\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size = 256, shuffle = True)\n",
    "    print(dataloader)\n",
    "    for i,data in enumerate(dataloader):\n",
    "        \n",
    "        print(data['emotion_label_idx'])\n",
    "        print(data['scene_label_idx'])\n",
    "        print(data['facial_expression_label_idx'])\n",
    "        print(data['human_action_label_idx'])\n",
    "        print(data['brightness_label_idx'])\n",
    "        print(data['colorfulness_label_idx'])\n",
    "        print(data['object_label_idx'])\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import os\n",
    "\n",
    "class EmotionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_emotions=8, hidden_dim=128):\n",
    "        super(EmotionRecognitionModel, self).__init__()\n",
    "        \n",
    "       \n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  \n",
    "        self.image_fc = nn.Linear(resnet.fc.in_features, hidden_dim)  \n",
    "        \n",
    "       \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "       \n",
    "        self.classifier = nn.Linear(hidden_dim, num_emotions) \n",
    "\n",
    "    def forward(self, image):\n",
    "        \n",
    "        image_features = self.cnn(image)  \n",
    "        image_features = image_features.view(image_features.size(0), -1)  \n",
    "        image_features = self.image_fc(image_features)  \n",
    "        \n",
    "    \n",
    "        image_features = self.dropout(image_features)\n",
    "        \n",
    "      \n",
    "        output = self.classifier(image_features)  \n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tanaz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Tanaz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n",
      "Training...\n",
      "Batch 0, Loss: 2.1188, Accuracy: 12.50%\n",
      "Batch 20, Loss: 2.3041, Accuracy: 25.00%\n",
      "Batch 40, Loss: 2.1736, Accuracy: 12.50%\n",
      "Batch 60, Loss: 1.9714, Accuracy: 15.62%\n",
      "Batch 80, Loss: 1.9335, Accuracy: 18.75%\n",
      "Batch 100, Loss: 1.7839, Accuracy: 21.88%\n",
      "Batch 120, Loss: 2.0235, Accuracy: 21.88%\n",
      "Batch 140, Loss: 1.6135, Accuracy: 31.25%\n",
      "Batch 160, Loss: 1.9593, Accuracy: 28.12%\n",
      "Batch 180, Loss: 1.8112, Accuracy: 34.38%\n",
      "Batch 200, Loss: 1.9929, Accuracy: 21.88%\n",
      "Batch 220, Loss: 1.7863, Accuracy: 28.12%\n",
      "Batch 240, Loss: 1.8841, Accuracy: 21.88%\n",
      "Batch 260, Loss: 1.6894, Accuracy: 34.38%\n",
      "Batch 280, Loss: 1.9629, Accuracy: 18.75%\n",
      "Batch 300, Loss: 1.5842, Accuracy: 50.00%\n",
      "Batch 320, Loss: 1.9542, Accuracy: 25.00%\n",
      "Batch 340, Loss: 1.6473, Accuracy: 37.50%\n",
      "Batch 360, Loss: 1.7395, Accuracy: 31.25%\n",
      "Batch 380, Loss: 1.9867, Accuracy: 21.88%\n",
      "Batch 400, Loss: 1.4335, Accuracy: 53.12%\n",
      "Batch 420, Loss: 1.7422, Accuracy: 34.38%\n",
      "Batch 440, Loss: 1.7070, Accuracy: 34.38%\n",
      "Batch 460, Loss: 1.7609, Accuracy: 18.75%\n",
      "Batch 480, Loss: 1.7250, Accuracy: 31.25%\n",
      "Batch 500, Loss: 1.5678, Accuracy: 50.00%\n",
      "Batch 520, Loss: 1.5381, Accuracy: 43.75%\n",
      "Batch 540, Loss: 1.6873, Accuracy: 40.62%\n",
      "Batch 560, Loss: 1.5865, Accuracy: 40.62%\n",
      "Batch 580, Loss: 1.6264, Accuracy: 40.62%\n",
      "Batch 600, Loss: 1.8727, Accuracy: 37.50%\n",
      "Batch 620, Loss: 1.7094, Accuracy: 40.62%\n",
      "Batch 640, Loss: 1.8208, Accuracy: 25.00%\n",
      "Batch 660, Loss: 1.6362, Accuracy: 31.25%\n",
      "Batch 680, Loss: 1.9026, Accuracy: 40.62%\n",
      "Batch 700, Loss: 1.5926, Accuracy: 40.62%\n",
      "Batch 720, Loss: 1.8430, Accuracy: 28.12%\n",
      "Batch 740, Loss: 1.7898, Accuracy: 40.62%\n",
      "Batch 760, Loss: 1.6725, Accuracy: 31.25%\n",
      "Batch 780, Loss: 1.7762, Accuracy: 28.12%\n",
      "Batch 800, Loss: 1.7490, Accuracy: 34.38%\n",
      "Batch 820, Loss: 1.7983, Accuracy: 37.50%\n",
      "Batch 840, Loss: 1.7987, Accuracy: 40.62%\n",
      "Batch 860, Loss: 2.1133, Accuracy: 15.62%\n",
      "Batch 880, Loss: 1.3822, Accuracy: 50.00%\n",
      "Batch 900, Loss: 1.8504, Accuracy: 25.00%\n",
      "Batch 920, Loss: 1.4474, Accuracy: 43.75%\n",
      "Batch 940, Loss: 1.6636, Accuracy: 37.50%\n",
      "Batch 960, Loss: 1.6757, Accuracy: 34.38%\n",
      "Batch 980, Loss: 1.8023, Accuracy: 43.75%\n",
      "Batch 1000, Loss: 1.6174, Accuracy: 28.12%\n",
      "Batch 1020, Loss: 1.6727, Accuracy: 31.25%\n",
      "Batch 1040, Loss: 1.6694, Accuracy: 37.50%\n",
      "Batch 1060, Loss: 1.8652, Accuracy: 25.00%\n",
      "Batch 1080, Loss: 1.5734, Accuracy: 31.25%\n",
      "Batch 1100, Loss: 1.5088, Accuracy: 40.62%\n",
      "Batch 1120, Loss: 1.7691, Accuracy: 31.25%\n",
      "Batch 1140, Loss: 1.6867, Accuracy: 40.62%\n",
      "Batch 1160, Loss: 1.6520, Accuracy: 37.50%\n",
      "Batch 1180, Loss: 1.6898, Accuracy: 31.25%\n",
      "Batch 1200, Loss: 1.7181, Accuracy: 46.88%\n",
      "Batch 1220, Loss: 1.6512, Accuracy: 40.62%\n",
      "Batch 1240, Loss: 1.7943, Accuracy: 31.25%\n",
      "Batch 1260, Loss: 1.4832, Accuracy: 56.25%\n",
      "Batch 1280, Loss: 1.7669, Accuracy: 34.38%\n",
      "Batch 1300, Loss: 1.6662, Accuracy: 40.62%\n",
      "Batch 1320, Loss: 1.8983, Accuracy: 28.12%\n",
      "Batch 1340, Loss: 1.6770, Accuracy: 31.25%\n",
      "Batch 1360, Loss: 1.4847, Accuracy: 37.50%\n",
      "Batch 1380, Loss: 1.5780, Accuracy: 40.62%\n",
      "Batch 1400, Loss: 1.9063, Accuracy: 28.12%\n",
      "Batch 1420, Loss: 1.6721, Accuracy: 34.38%\n",
      "Batch 1440, Loss: 1.9002, Accuracy: 40.62%\n",
      "Batch 1460, Loss: 1.5771, Accuracy: 40.62%\n",
      "Batch 1480, Loss: 1.3465, Accuracy: 43.75%\n",
      "Batch 1500, Loss: 1.4969, Accuracy: 43.75%\n",
      "Batch 1520, Loss: 1.7022, Accuracy: 34.38%\n",
      "Batch 1540, Loss: 1.6329, Accuracy: 40.62%\n",
      "Batch 1560, Loss: 1.6714, Accuracy: 25.00%\n",
      "Batch 1580, Loss: 2.0472, Accuracy: 28.12%\n",
      "Batch 1600, Loss: 1.7282, Accuracy: 37.50%\n",
      "Batch 1620, Loss: 1.7884, Accuracy: 28.12%\n",
      "Batch 1640, Loss: 1.6078, Accuracy: 37.50%\n",
      "Batch 1660, Loss: 1.5082, Accuracy: 43.75%\n",
      "Batch 1680, Loss: 1.5746, Accuracy: 40.62%\n",
      "Batch 1700, Loss: 1.7706, Accuracy: 40.62%\n",
      "Batch 1720, Loss: 1.5098, Accuracy: 56.25%\n",
      "Batch 1740, Loss: 1.4817, Accuracy: 50.00%\n",
      "Batch 1760, Loss: 1.5555, Accuracy: 31.25%\n",
      "Batch 1780, Loss: 1.4406, Accuracy: 43.75%\n",
      "Batch 1800, Loss: 1.6647, Accuracy: 28.12%\n",
      "Batch 1820, Loss: 1.4458, Accuracy: 37.50%\n",
      "Batch 1840, Loss: 1.5720, Accuracy: 53.12%\n",
      "Batch 1860, Loss: 1.4935, Accuracy: 46.88%\n",
      "Batch 1880, Loss: 1.7732, Accuracy: 34.38%\n",
      "Batch 1900, Loss: 1.5858, Accuracy: 34.38%\n",
      "Batch 1920, Loss: 1.9566, Accuracy: 31.25%\n",
      "Batch 1940, Loss: 1.4924, Accuracy: 50.00%\n",
      "Batch 1960, Loss: 1.6680, Accuracy: 43.75%\n",
      "Batch 1980, Loss: 1.5986, Accuracy: 31.25%\n",
      "Batch 2000, Loss: 1.4307, Accuracy: 43.75%\n",
      "Batch 2020, Loss: 1.5644, Accuracy: 37.50%\n",
      "Batch 2040, Loss: 1.5302, Accuracy: 59.38%\n",
      "Batch 2060, Loss: 1.4436, Accuracy: 50.00%\n",
      "Batch 2080, Loss: 1.8628, Accuracy: 31.25%\n",
      "Batch 2100, Loss: 1.4500, Accuracy: 50.00%\n",
      "Batch 2120, Loss: 1.6080, Accuracy: 43.75%\n",
      "Batch 2140, Loss: 2.0021, Accuracy: 34.38%\n",
      "Batch 2160, Loss: 1.5052, Accuracy: 40.62%\n",
      "Batch 2180, Loss: 1.6025, Accuracy: 50.00%\n",
      "Batch 2200, Loss: 1.6569, Accuracy: 40.62%\n",
      "Batch 2220, Loss: 1.4365, Accuracy: 50.00%\n",
      "Batch 2240, Loss: 1.4351, Accuracy: 46.88%\n",
      "Batch 2260, Loss: 1.4270, Accuracy: 34.38%\n",
      "Batch 2280, Loss: 1.5791, Accuracy: 37.50%\n",
      "Batch 2300, Loss: 1.6911, Accuracy: 31.25%\n",
      "Batch 2320, Loss: 1.6656, Accuracy: 40.62%\n",
      "Batch 2340, Loss: 1.4851, Accuracy: 59.38%\n",
      "Batch 2360, Loss: 1.4433, Accuracy: 46.88%\n",
      "Batch 2380, Loss: 1.5391, Accuracy: 46.88%\n",
      "Batch 2400, Loss: 1.8601, Accuracy: 31.25%\n",
      "Batch 2420, Loss: 1.5675, Accuracy: 31.25%\n",
      "Batch 2440, Loss: 1.7986, Accuracy: 25.00%\n",
      "Batch 2460, Loss: 1.5898, Accuracy: 40.62%\n",
      "Batch 2480, Loss: 1.9063, Accuracy: 37.50%\n",
      "Batch 2500, Loss: 1.4927, Accuracy: 37.50%\n",
      "Batch 2520, Loss: 1.7876, Accuracy: 37.50%\n",
      "Batch 2540, Loss: 1.7317, Accuracy: 46.88%\n",
      "Batch 2560, Loss: 1.5495, Accuracy: 50.00%\n",
      "Batch 2580, Loss: 1.5973, Accuracy: 37.50%\n",
      "Batch 2600, Loss: 1.4045, Accuracy: 50.00%\n",
      "Batch 2620, Loss: 1.2941, Accuracy: 53.12%\n",
      "Batch 2640, Loss: 1.7078, Accuracy: 25.00%\n",
      "Batch 2660, Loss: 1.6047, Accuracy: 37.50%\n",
      "Batch 2680, Loss: 1.5234, Accuracy: 46.88%\n",
      "Batch 2700, Loss: 1.6458, Accuracy: 46.88%\n",
      "Batch 2720, Loss: 1.7346, Accuracy: 43.75%\n",
      "Batch 2740, Loss: 1.6696, Accuracy: 31.25%\n",
      "Batch 2760, Loss: 1.5336, Accuracy: 43.75%\n",
      "Batch 2780, Loss: 1.6046, Accuracy: 46.88%\n",
      "Batch 2800, Loss: 1.6760, Accuracy: 46.88%\n",
      "Batch 2820, Loss: 1.6199, Accuracy: 46.88%\n",
      "Batch 2840, Loss: 1.4482, Accuracy: 40.62%\n",
      "Batch 2860, Loss: 1.2867, Accuracy: 46.88%\n",
      "Batch 2880, Loss: 1.4798, Accuracy: 50.00%\n",
      "Batch 2900, Loss: 1.8233, Accuracy: 34.38%\n",
      "Batch 2920, Loss: 1.4698, Accuracy: 43.75%\n",
      "Batch 2940, Loss: 1.7264, Accuracy: 34.38%\n",
      "Training - Loss: 1.6943, Accuracy: 37.49%\n",
      "Validating...\n",
      "Validation - Loss: 1.4663, Accuracy: 46.94%\n",
      "Model saved at: checkpoints\\emotion_recognition_image_only_epoch_1.pth\n",
      "New best model saved with accuracy: 46.94%\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n",
      "Training...\n",
      "Batch 0, Loss: 1.5394, Accuracy: 40.62%\n",
      "Batch 20, Loss: 1.7066, Accuracy: 43.75%\n",
      "Batch 40, Loss: 1.7559, Accuracy: 43.75%\n",
      "Batch 60, Loss: 1.4061, Accuracy: 43.75%\n",
      "Batch 80, Loss: 1.7450, Accuracy: 34.38%\n",
      "Batch 100, Loss: 1.6690, Accuracy: 37.50%\n",
      "Batch 120, Loss: 1.6200, Accuracy: 37.50%\n",
      "Batch 140, Loss: 1.5006, Accuracy: 40.62%\n",
      "Batch 160, Loss: 1.6871, Accuracy: 28.12%\n",
      "Batch 180, Loss: 1.5607, Accuracy: 37.50%\n",
      "Batch 200, Loss: 1.5100, Accuracy: 37.50%\n",
      "Batch 220, Loss: 1.2670, Accuracy: 65.62%\n",
      "Batch 240, Loss: 1.4044, Accuracy: 46.88%\n",
      "Batch 260, Loss: 1.3989, Accuracy: 43.75%\n",
      "Batch 280, Loss: 1.4150, Accuracy: 46.88%\n",
      "Batch 300, Loss: 1.7850, Accuracy: 28.12%\n",
      "Batch 320, Loss: 1.2206, Accuracy: 53.12%\n",
      "Batch 340, Loss: 1.4976, Accuracy: 46.88%\n",
      "Batch 360, Loss: 1.3689, Accuracy: 50.00%\n",
      "Batch 380, Loss: 1.4803, Accuracy: 40.62%\n",
      "Batch 400, Loss: 1.6364, Accuracy: 37.50%\n",
      "Batch 420, Loss: 1.5758, Accuracy: 34.38%\n",
      "Batch 440, Loss: 1.3801, Accuracy: 40.62%\n",
      "Batch 460, Loss: 1.6002, Accuracy: 37.50%\n",
      "Batch 480, Loss: 1.5486, Accuracy: 43.75%\n",
      "Batch 500, Loss: 1.6924, Accuracy: 34.38%\n",
      "Batch 520, Loss: 1.3207, Accuracy: 59.38%\n",
      "Batch 540, Loss: 1.4781, Accuracy: 40.62%\n",
      "Batch 560, Loss: 1.2789, Accuracy: 46.88%\n",
      "Batch 580, Loss: 1.4328, Accuracy: 46.88%\n",
      "Batch 600, Loss: 1.4231, Accuracy: 56.25%\n",
      "Batch 620, Loss: 1.5507, Accuracy: 46.88%\n",
      "Batch 640, Loss: 1.3861, Accuracy: 43.75%\n",
      "Batch 660, Loss: 1.3980, Accuracy: 46.88%\n",
      "Batch 680, Loss: 1.6605, Accuracy: 37.50%\n",
      "Batch 700, Loss: 1.4843, Accuracy: 46.88%\n",
      "Batch 720, Loss: 1.3185, Accuracy: 50.00%\n",
      "Batch 740, Loss: 1.5972, Accuracy: 34.38%\n",
      "Batch 760, Loss: 1.4959, Accuracy: 50.00%\n",
      "Batch 780, Loss: 1.4444, Accuracy: 46.88%\n",
      "Batch 800, Loss: 1.4510, Accuracy: 53.12%\n",
      "Batch 820, Loss: 1.3675, Accuracy: 46.88%\n",
      "Batch 840, Loss: 1.2908, Accuracy: 59.38%\n",
      "Batch 860, Loss: 1.3664, Accuracy: 65.62%\n",
      "Batch 880, Loss: 1.3937, Accuracy: 53.12%\n",
      "Batch 900, Loss: 1.4887, Accuracy: 40.62%\n",
      "Batch 920, Loss: 1.3645, Accuracy: 43.75%\n",
      "Batch 940, Loss: 1.4065, Accuracy: 53.12%\n",
      "Batch 960, Loss: 1.4358, Accuracy: 53.12%\n",
      "Batch 980, Loss: 1.3235, Accuracy: 56.25%\n",
      "Batch 1000, Loss: 1.4926, Accuracy: 40.62%\n",
      "Batch 1020, Loss: 1.7896, Accuracy: 34.38%\n",
      "Error in training batch 1023: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1024: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1025: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1026: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1027: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1028: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n",
      "Error in training batch 1029: CUDA error: an illegal memory access was encountered\n",
      "CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\n",
      "For debugging consider passing CUDA_LAUNCH_BLOCKING=1\n",
      "Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_batch(model, batch, device):\n",
    "    \n",
    "    images = batch['image'].to(device)\n",
    "    emotion_labels = batch['emotion_label_idx'].to(device)\n",
    "    outputs = model(images)\n",
    "    \n",
    "    return outputs, emotion_labels\n",
    "\n",
    "def train_and_validate(model, train_dataloader, valid_dataloader, criterion, optimizer, num_epochs=10, device='cuda', save_dir='checkpoints'):\n",
    "    model.to(device)\n",
    "    \n",
    "   \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    \n",
    " \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    valid_losses = []\n",
    "    valid_accuracies = []\n",
    "    \n",
    "    best_valid_acc = 0.0\n",
    "    best_model_wts = model.state_dict()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print('-' * 30)\n",
    "        \n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        print(\"Training...\")\n",
    "        for i, batch in enumerate(train_dataloader):\n",
    "            try:\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                outputs, emotion_labels = process_batch(model, batch, device)\n",
    "                loss = criterion(outputs, emotion_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                \n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_predictions += emotion_labels.size(0)\n",
    "                correct_predictions += (predicted == emotion_labels).sum().item()\n",
    "                \n",
    "                if i % 20 == 0:\n",
    "                    batch_accuracy = 100 * (predicted == emotion_labels).sum().item() / emotion_labels.size(0)\n",
    "                    print(f\"Batch {i}, Loss: {loss.item():.4f}, Accuracy: {batch_accuracy:.2f}%\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in training batch {i}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        train_epoch_loss = running_loss / len(train_dataloader)\n",
    "        train_epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "        \n",
    "        train_losses.append(train_epoch_loss)\n",
    "        train_accuracies.append(train_epoch_accuracy)\n",
    "        \n",
    "        print(f\"Training - Loss: {train_epoch_loss:.4f}, Accuracy: {train_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "      \n",
    "        model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        print(\"Validating...\")\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valid_dataloader):\n",
    "                try:\n",
    "                    outputs, emotion_labels = process_batch(model, batch, device)\n",
    "                    loss = criterion(outputs, emotion_labels)\n",
    "                    \n",
    "                    running_loss += loss.item()\n",
    "                    \n",
    "                    \n",
    "                    _, predicted = torch.max(outputs.data, 1)\n",
    "                    total_predictions += emotion_labels.size(0)\n",
    "                    correct_predictions += (predicted == emotion_labels).sum().item()\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch {i}: {e}\")\n",
    "                    continue\n",
    "   \n",
    "        valid_epoch_loss = running_loss / len(valid_dataloader)\n",
    "        valid_epoch_accuracy = 100 * correct_predictions / total_predictions\n",
    "        \n",
    "        valid_losses.append(valid_epoch_loss)\n",
    "        valid_accuracies.append(valid_epoch_accuracy)\n",
    "        \n",
    "        print(f\"Validation - Loss: {valid_epoch_loss:.4f}, Accuracy: {valid_epoch_accuracy:.2f}%\")\n",
    "        \n",
    "        \n",
    "        checkpoint = {\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_epoch_loss,\n",
    "            'train_acc': train_epoch_accuracy,\n",
    "            'valid_loss': valid_epoch_loss,\n",
    "            'valid_acc': valid_epoch_accuracy,\n",
    "            'best_valid_acc': best_valid_acc\n",
    "        }\n",
    "        checkpoint_path = os.path.join(save_dir, f'emotion_recognition_image_only_epoch_{epoch+1}.pth')\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Model saved at: {checkpoint_path}\")\n",
    "        \n",
    "       \n",
    "        if valid_epoch_accuracy > best_valid_acc:\n",
    "            best_valid_acc = valid_epoch_accuracy\n",
    "            best_model_wts = model.state_dict().copy()\n",
    "            print(f\"New best model saved with accuracy: {best_valid_acc:.2f}%\")\n",
    "    \n",
    "    \n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"Training complete! Best validation accuracy: {best_valid_acc:.2f}%\")\n",
    "    \n",
    "    \n",
    "    history = {\n",
    "        'train_loss': train_losses,\n",
    "        'train_acc': train_accuracies,\n",
    "        'valid_loss': valid_losses,\n",
    "        'valid_acc': valid_accuracies,\n",
    "        'best_acc': best_valid_acc\n",
    "    }\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    \n",
    "    data_root = \"dataset\"\n",
    "    num_emotion_classes = 8\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "  \n",
    "    train_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='train')\n",
    "    valid_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='val')\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    \n",
    "    model = EmotionRecognitionModel(num_emotions=num_emotion_classes, hidden_dim=128).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "   \n",
    "    trained_model, history = train_and_validate(\n",
    "        model, train_dataloader, valid_dataloader, criterion, optimizer, \n",
    "        num_epochs=10, device=device, save_dir='checkpoints'\n",
    "    )\n",
    "    \n",
    "   \n",
    "    torch.save(trained_model.state_dict(), \"emotion_recognition_image_only_best.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = EmoSet(data_root=data_root, num_emotion_classes=num_emotion_classes, phase='test')\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Load trained model\n",
    "model = EmotionRecognitionModel(num_emotions=num_emotion_classes, hidden_dim=128).to(device)\n",
    "model.load_state_dict(torch.load(\"checkpoints/emotion_recognition_image_only_epoch_1.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "# Evaluate\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        outputs, emotion_labels = process_batch(model, batch, device)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(emotion_labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "emotion_labels = ['amusement', 'anger', 'awe', 'contentment', 'disgust', 'excitement', 'fear', 'sadness']\n",
    "print(\"Image-Only Emotion Recognition Classification Report:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=emotion_labels, zero_division=0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
